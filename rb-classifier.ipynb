{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf09889",
   "metadata": {},
   "source": [
    "# RuleBased Classification of QA dataset\n",
    "Load in libs, dataframe and examine answerable=true proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from typing import Any\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = pl.read_parquet(ARB_CACHE)\n",
    "df_ko = pl.read_parquet(KOR_CACHE)\n",
    "df_te = pl.read_parquet(TELU_CACHE)\n",
    "df_arkote = pl.concat([df_ar, df_ko, df_te])\n",
    "assert df_ar.height + df_ko.height + df_te.height == df_arkote.height; # sanity check\n",
    "print(f\"Answerable proportion: {df_arkote['answerable'].sum() / df_arkote.height:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41157d3e",
   "metadata": {},
   "source": [
    "### Expression based approach to making rule based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_positive_rule() -> pl.Expr:\n",
    "    # Guessing positive in all cases as a baseline\n",
    "    return pl.lit(True)\n",
    "\n",
    "\n",
    "def when_rule() -> pl.Expr:\n",
    "    return pl.col(\"translation\").str.to_lowercase().str.contains(\"when\") & pl.col(\"context\").str.contains(r\"\\d\")\n",
    "\n",
    "def although_rule() -> pl.Expr:\n",
    "    return ~pl.col(\"context\").str.to_lowercase().str.contains(\"although\")\n",
    "\n",
    "def does_rule() -> pl.Expr:\n",
    "    return ~pl.col(\"translation\").str.to_lowercase().str.contains(\"does\")\n",
    "\n",
    "def first_letter_rule() -> pl.Expr:\n",
    "    return pl.col(\"translation\").str.slice(0, 1).str.to_lowercase().is_in([\"w\", \"h\"])\n",
    "\n",
    "def rule_based_classification() -> pl.Expr:\n",
    "    return (although_rule() & does_rule()) | (when_rule() | first_letter_rule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eddb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_performance(df_: pl.DataFrame, title: str = \"Normalized Confusion Matrix (Row-wise)\"):\n",
    "    # Proportion of answerable predictions\n",
    "    print(f\"Answerable proportion: {df_['answerable'].sum() / df_.height:.2f}\")\n",
    "    print(f\"Predicted answerable proportion: {df_['answerable_pred'].sum() / df_.height:.2f}\")\n",
    "\n",
    "    # Confusion matrix counts\n",
    "    tp = df_.filter(pl.col(\"answerable\") & pl.col(\"answerable_pred\")).height \n",
    "    fp = df_.filter(~pl.col(\"answerable\") & pl.col(\"answerable_pred\")).height\n",
    "    tn = df_.filter(~pl.col(\"answerable\") & ~pl.col(\"answerable_pred\")).height\n",
    "    fn = df_.filter(pl.col(\"answerable\") & ~pl.col(\"answerable_pred\")).height\n",
    "\n",
    "    y_true = df_[\"answerable\"].to_list()\n",
    "    y_pred = df_[\"answerable_pred\"].to_list()\n",
    "\n",
    "    print(f\"True Positives: {tp} , False Positives: {fp}\")\n",
    "    print(f\"True Negatives: {tn} , False Negatives: {fn}\")\n",
    "\n",
    "    # Accuracy, Precision, Recall, F1 Score\n",
    "    accuracy = (tp + tn) / df_.height\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "    # normlized confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true') # row normalization\n",
    "    plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['N', 'A'])\n",
    "    plt.yticks(tick_marks, ['N', 'A'])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    # Add text annotations for each cell in the confusion matrix\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e96f8",
   "metadata": {},
   "source": [
    "### Performance of rules based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f98e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of each individual rule\n",
    "print(\"JUST POSITIVE RULE PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    just_positive_rule().alias(\"answerable_pred\")\n",
    "), title=\"Just Positive Rule\")\n",
    "print(\"ALTHOUGH RULE PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    although_rule().alias(\"answerable_pred\")\n",
    "), title=\"Although Rule\")\n",
    "print(\"DOES RULE PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    does_rule().alias(\"answerable_pred\")\n",
    "), title=\"Does Rule\")\n",
    "print(\"FIRST LETTER RULE PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    first_letter_rule().alias(\"answerable_pred\")\n",
    "), title=\"First Letter Rule\")\n",
    "print(\"WHEN RULE PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    when_rule().alias(\"answerable_pred\")\n",
    "), title=\"When Rule\")\n",
    "print(\"COMBINED RULES PERFORMANCE\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    rule_based_classification().alias(\"answerable_pred\")\n",
    "), title=\"Combined Rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined dataset performance\")\n",
    "show_performance(df_arkote.with_columns(\n",
    "    rule_based_classification().alias(\"answerable_pred\")\n",
    "), title=\"Combined Dataset\")\n",
    "print(\"Arabic dataset performance\")\n",
    "show_performance(df_ar.with_columns(\n",
    "    rule_based_classification().alias(\"answerable_pred\")\n",
    "), title=\"Arabic Dataset\")\n",
    "print(\"Korean dataset performance\")\n",
    "show_performance(df_ko.with_columns(\n",
    "    rule_based_classification().alias(\"answerable_pred\")\n",
    "), title=\"Korean Dataset\")\n",
    "print(\"Telugu dataset performance\")\n",
    "show_performance(df_te.with_columns(\n",
    "    rule_based_classification().alias(\"answerable_pred\")\n",
    "), title=\"Telugu Dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
