{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643ea4c",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "from typing import TypeAlias\n",
    "from ngrams.utils import (\n",
    "    TokenizedSentences,\n",
    "    NGramsDict,\n",
    "    DataInconsistencyError,\n",
    "    train_test_split_and_tokenize,\n",
    "    get_ngrams_dict,\n",
    "    get_ngrams_dict_from_sentences,\n",
    "    tokenize,\n",
    ")\n",
    "from ngrams.models import NGramLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arkote = pl.concat([\n",
    "    pl.read_parquet(ARB_CACHE),\n",
    "    pl.read_parquet(KOR_CACHE),\n",
    "    pl.read_parquet(TELU_CACHE)\n",
    "])\n",
    "df_ko_mini = pl.read_parquet(KOR_CACHE)[:100]\n",
    "df_ko_mini.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ko_mini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf7c8",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4a02",
   "metadata": {},
   "source": [
    "### Examine the corpus stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average context length\n",
    "context = df_arkote[\"context\"]\n",
    "avg_len = sum(len(c) for c in context) / len(context)\n",
    "print(f\"Average context length: {avg_len:.2f} characters\")\n",
    "print(f\"Number of sequences (rows) in context: {len(context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english corpus\n",
    "context_corpus = df_arkote[\"context\"].to_list()\n",
    "# Get number of unique space seperated words (not tokens)\n",
    "context_vocab = set(\" \".join(context_corpus).split())\n",
    "number_of_unique_words = len(context_vocab)\n",
    "print(f\"Number of unique (space seperated) words in context: {number_of_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898097cd",
   "metadata": {},
   "source": [
    "### Tokenize corpus\n",
    "Here we use Multilingual BERT tokenizer. We use identical tokenizer for comparing perplexity.\n",
    "Each string entrance may be several sentences, but for simplicity we are gonna treat each one as a single sequence, and use the inherent start- and end-of-sentence markers from mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multilingual bert tokenizer\n",
    "mbert = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "mbert.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "\n",
    "# Example tokenization\n",
    "sample_content_tokens = tokenize(df_arkote[\"context\"][10], n=5)\n",
    "print(f\"Sample content tokens (n=5):\")\n",
    "\" | \".join(sample_content_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de37518",
   "metadata": {},
   "source": [
    "## N-Gram LM\n",
    "First we explore some statistics of $n$, to pick the size we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eea4a",
   "metadata": {},
   "source": [
    "### Examine NGramDicts for context series for N={1, 2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unigrams for train\n",
    "n1_context_train, n1_context_test = train_test_split_and_tokenize(context, verbose=True)\n",
    "print(\"Getting unigrams...\")\n",
    "unigrams = get_ngrams_dict_from_sentences(n1_context_train, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a047453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bigrams for train\n",
    "n2_context_train, n2_context_test = train_test_split_and_tokenize(context, n=2, verbose=True)\n",
    "print(\"Getting bigrams...\")\n",
    "bigrams = get_ngrams_dict_from_sentences(n2_context_train, 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine trigrams for train\n",
    "n3_context_train, n3_context_test = train_test_split_and_tokenize(context, n=3, verbose=True)\n",
    "print(\"Getting trigrams...\")\n",
    "trigrams = get_ngrams_dict_from_sentences(n3_context_train, 3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine 4-grams for train\n",
    "n4_context_train, n4_context_test = train_test_split_and_tokenize(context, n=4, verbose=True)\n",
    "print(\"Getting fourgrams...\")\n",
    "fourgrams = get_ngrams_dict_from_sentences(n4_context_train, 4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68133a36",
   "metadata": {},
   "source": [
    "### NGramModel from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "NGram = tuple[str, ...]\n",
    "Tokens: TypeAlias = list[str]\n",
    "TokenizedSentences: TypeAlias = list[Tokens]\n",
    "\n",
    "class NGramLM:\n",
    "    \"\"\"Class to represent an N-gram language model.\n",
    "    The model takes as input the n-grams and (n-1)-grams count dictionaries,\n",
    "    and computes the conditional probabilities of the n-grams given the (n-1)-grams. \n",
    "    \"\"\"\n",
    "    def __init__(self, nm1grams: NGramsDict, ngrams: NGramsDict,\n",
    "                 vocabulary: set[str] | None = None, smoothing: str | None =\"laplace\"):\n",
    "        self.ngrams = ngrams\n",
    "        self.nm1grams = nm1grams\n",
    "        self.n = len(list(ngrams.keys())[0])\n",
    "        if self.n <= 1:\n",
    "            raise ValueError(\"'n' for n-grams must be greater than 1.\")\n",
    "        if not vocabulary:\n",
    "            self.vocabulary = {token for nm1gram in self.nm1grams.keys() for token in nm1gram}\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        # Probabilities of words given their (n-1) prefix\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        # P(w_n | w_1 ... w_(n-1)) for each n-gram\n",
    "        self.probabilities = {key: {} for key in self.nm1grams.keys()}\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha = 1 if smoothing else 0.0\n",
    "        self._calc_word_probabilities()\n",
    "\n",
    "    def _calc_word_probabilities(self) -> None:\n",
    "        \"\"\"Given an (n-1)gram we want the probability distribution over every possible next word\"\"\" \n",
    "        for ngram in self.ngrams.keys():\n",
    "            word = ngram[-1]\n",
    "            prefix = ngram[:-1]\n",
    "            invalid_nm1gram = (\"<s>\",) * (self.n - 1) if self.n > 2 else None\n",
    "            if prefix == invalid_nm1gram:\n",
    "                continue # n*start token only exists for ngrams, not (n-1)grams\n",
    "            word_idx = self.word_to_idx[word]\n",
    "            ngram_count = self.ngrams.get(ngram, 0) \n",
    "            nm1gram_count = self.nm1grams.get(prefix, 0)\n",
    "            if self.smoothing == \"laplace\":\n",
    "                ngram_count += self.alpha\n",
    "                nm1gram_count += self.alpha * self.vocab_size\n",
    "            self.probabilities[prefix][word_idx] = ngram_count / nm1gram_count\n",
    "    \n",
    "    def get_word_probability(self, nm1gram: NGram, word: str) -> float:\n",
    "        \"\"\"Get the probability of a word given its (n-1)-gram prefix.\"\"\"\n",
    "        if len(nm1gram) != self.n - 1:\n",
    "            raise ValueError(f\"Key must be of length {self.n - 1} i.e. an (n-1)-gram.\")\n",
    "        if nm1gram not in self.probabilities:\n",
    "            raise KeyError(f\"(n-1)-gram {nm1gram} not found in model.\")\n",
    "        if word not in self.word_to_idx:\n",
    "            raise KeyError(f\"Word {word} not found in vocabulary.\")\n",
    "        word_idx = self.word_to_idx[word]\n",
    "        return self.probabilities[nm1gram][word_idx]\n",
    "\n",
    "    def get_word_distribution(self, nm1gram: NGram) -> dict[int, float]:\n",
    "        \"\"\"Get the probability distribution for a given (n-1)-gram.\"\"\"\n",
    "        if len(nm1gram) != self.n - 1:\n",
    "            raise ValueError(f\"Key must be of length {self.n - 1} i.e. an (n-1)-gram.\")\n",
    "        if nm1gram not in self.probabilities:\n",
    "            raise KeyError(f\"(n-1)-gram {nm1gram} not found in model.\")\n",
    "        return self.probabilities[nm1gram]\n",
    "\n",
    "    def get_sentence_probability(self, sentence: list[NGram], verbose=False) -> float:\n",
    "        \"\"\"Get the probability of a sentence (list of tokens) under this model.\"\"\"\n",
    "        log_prob = 0.0\n",
    "        assert len(sentence[0]) == self.n, \"Each ngram in sentence must be of length n\"\n",
    "        for ngram in sentence:\n",
    "            word = ngram[-1]\n",
    "            nm1gram = ngram[:-1]\n",
    "            word_idx = self.word_to_idx.get(word, self.vocab_size + 1)\n",
    "            prob_dict = self.probabilities.get(nm1gram, {})\n",
    "            default_prob = self.alpha / (self.alpha * self.vocab_size) if self.smoothing else 0.0\n",
    "            prob = prob_dict.get(word_idx, default_prob)\n",
    "            if verbose:\n",
    "                print(f\"P({ngram[-1]}|{ngram[:-1]}) = {prob:.2f}\")\n",
    "            if self.smoothing:\n",
    "                assert prob > 0.0, \"Smoothing method should eliminate zero probabilities\"\n",
    "            if prob == 0.0 and verbose:\n",
    "                print(f\"WARNING: Zero probability for ngram {ngram}.\")\n",
    "            log_prob += math.log(prob) if prob > 0.0 else 0.0\n",
    "    \n",
    "        return math.exp(log_prob)\n",
    "\n",
    "    def get_perplexity(self, sentences: list[list[NGram]]) -> float:\n",
    "        \"\"\"Get the perplexity of the model, on an unseen test set\"\"\"\n",
    "        # Get the total number of words. Counting </s> but not <s>:\n",
    "        N = sum([len(sentence) for sentence in sentences])\n",
    "        full_document_prob = sum([self.get_sentence_probability(sentence) for sentence in sentences])\n",
    "        if full_document_prob == 0.0:\n",
    "            print(\"WARNING: Zero document probability.\")\n",
    "            return 0.0\n",
    "        return math.pow(full_document_prob, -(1/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115791",
   "metadata": {},
   "source": [
    "## Verifying correctness of NGramModel\n",
    "First we regenerate probabilities from example in SLP book, then we verify our model against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_series = pl.Series([\"I am Sam\", \"Sam I am\", \"I do not like green eggs and ham\"])\n",
    "cased_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "cased_tokenizer.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "ngram_ready_tokens = [tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=2) for seq in mock_series]\n",
    "mock_data = ngram_ready_tokens # work for both uni- and bigrams\n",
    "mock_nm1grams = get_ngrams_dict_from_sentences(mock_data, 1, verbose=True)\n",
    "mock_ngrams = get_ngrams_dict_from_sentences(mock_data, 2, verbose=True)\n",
    "for row in mock_data:\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b56fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGramLM(mock_nm1grams, mock_ngrams, smoothing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mock_sentence = \"I am Sam\"\n",
    "mock_sentence = tokenize(mock_sentence, tokenizer=cased_tokenizer.tokenize, n=2)\n",
    "bigram_mock_sentence = list(nltk.ngrams(mock_sentence, 2))\n",
    "# Should ignore \"b\" as it is OOV and return 2/3 * 2/3 * 1/2 * 1/2 = 1/9 = 0.1111\n",
    "print(bigram_model.get_sentence_probability(bigram_mock_sentence, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f790d",
   "metadata": {},
   "source": [
    "Count num words via ngrams (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get N for bi, tri and fourgrams\n",
    "#for n in [2, 3, 4]:\n",
    "#    ngram_ready_tokens = [tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=n) for seq in mock_series]\n",
    "#    ngram_mock_sentences = [list(nltk.ngrams(seq, n)) for seq in ngram_ready_tokens]\n",
    "#    print(f\"Sample ngram: {ngram_mock_sentences[0] if ngram_mock_sentences else None}\")\n",
    "#    # Should match num tokens excluding <s>\n",
    "#    print(f\"Len of sample ngram (sanity check): {len(ngram_mock_sentences[0])}\")\n",
    "#    N = sum([len(sentence) for sentence in ngram_mock_sentences]) - len(ngram_mock_sentences)\n",
    "#    print(f\"N for {n}-grams: {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity of mock example\n",
    "ngram_ready_tokens = [tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=2) for seq in mock_series]\n",
    "bigram_mock_sentences = [list(nltk.ngrams(seq, 2)) for seq in ngram_ready_tokens]\n",
    "# Mock sentence [\"<s>\", \"I\", \"am\", \"Sam\", \"</s>\"], seq prob = 1/9  (not counting <s>)\n",
    "# thus perplexity = (1/9)^(-1/4) = sqrt(3) approx 1.732\n",
    "bigram_model.get_perplexity([bigram_mock_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe3a93",
   "metadata": {},
   "source": [
    "## Get Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fa2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "from typing import TypeAlias, cast\n",
    "\n",
    "NGram = tuple[str, ...]\n",
    "Tokens: TypeAlias = list[str]\n",
    "TokenizedSentences: TypeAlias = list[Tokens]\n",
    "NGramsDict: TypeAlias = dict[NGram, int]\n",
    "ModelReadyData: TypeAlias = tuple[NGramsDict, NGramsDict, list[list[NGram]]]\n",
    "\n",
    "def get_model_ready_data(corpus: pl.Series, n: int) -> ModelReadyData:\n",
    "    \"\"\"One function to call on relevant series, to get NGramModel ready data\"\"\"\n",
    "    x_train, x_test = train_test_split_and_tokenize(corpus, n=n)\n",
    "    nm1grams_dict = get_ngrams_dict_from_sentences(x_train, n-1, verbose=False)\n",
    "    ngrams_dict = get_ngrams_dict_from_sentences(x_train, n, verbose=False)\n",
    "    # Make test into list of ngram sentences\n",
    "    x_test = list([list(nltk.ngrams(sentence, n)) for sentence in x_test])\n",
    "    x_test = cast(list[list[NGram]], x_test)  # Type hinting for clarity\n",
    "    data = cast(ModelReadyData, (nm1grams_dict, ngrams_dict, x_test))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram, bigram, test = get_model_ready_data(df_arkote[\"context\"], 2)\n",
    "content_bigram_model = NGramLM(unigram, bigram, smoothing=\"laplace\")\n",
    "content_model_perplexity = content_bigram_model.get_perplexity(test)\n",
    "print(f\"Content bigram LM perplexity: {content_model_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
