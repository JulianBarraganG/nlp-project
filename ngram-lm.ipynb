{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643ea4c",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "from typing import TypeAlias\n",
    "from ngrams.utils import (\n",
    "    TokenizedSentences,\n",
    "    NGramsDict,\n",
    "    DataInconsistencyError,\n",
    "    train_test_split_and_tokenize,\n",
    "    get_ngrams_dict,\n",
    "    get_ngrams_dict_from_sentences,\n",
    "    tokenize,\n",
    ")\n",
    "from ngrams.models import NGramLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arkote = pl.concat([\n",
    "    pl.read_parquet(ARB_CACHE),\n",
    "    pl.read_parquet(KOR_CACHE),\n",
    "    pl.read_parquet(TELU_CACHE)\n",
    "])\n",
    "df_ko_mini = pl.read_parquet(KOR_CACHE)[:100]\n",
    "df_ko_mini.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ko_mini.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf7c8",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4a02",
   "metadata": {},
   "source": [
    "### Examine the corpus stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average context length\n",
    "context = df_arkote[\"context\"]\n",
    "avg_len = sum(len(c) for c in context) / len(context)\n",
    "print(f\"Average context length: {avg_len:.2f} characters\")\n",
    "print(f\"Number of sequences (rows) in context: {len(context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english corpus\n",
    "context_corpus = df_arkote[\"context\"].to_list()\n",
    "# Get number of unique space seperated words (not tokens)\n",
    "context_vocab = set(\" \".join(context_corpus).split())\n",
    "number_of_unique_words = len(context_vocab)\n",
    "print(f\"Number of unique (space seperated) words in context: {number_of_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898097cd",
   "metadata": {},
   "source": [
    "### Tokenize corpus\n",
    "Here we use Multilingual BERT tokenizer. We use identical tokenizer for comparing perplexity.\n",
    "Each string entrance may be several sentences, but for simplicity we are gonna treat each one as a single sequence, and use the inherent start- and end-of-sentence markers from mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multilingual bert tokenizer\n",
    "mbert = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "mbert.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "\n",
    "# Example tokenization\n",
    "sample_content_tokens = tokenize(df_arkote[\"context\"][10], n=5)\n",
    "print(f\"Sample content tokens (n=5):\")\n",
    "\" | \".join(sample_content_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de37518",
   "metadata": {},
   "source": [
    "## N-Gram LM\n",
    "First we explore some statistics of $n$, to pick the size we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eea4a",
   "metadata": {},
   "source": [
    "### Examine NGramDicts for context series for N={1, 2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unigrams for train\n",
    "n1_context_train, n1_context_test = train_test_split_and_tokenize(context, verbose=True)\n",
    "print(\"Getting unigrams...\")\n",
    "unigrams = get_ngrams_dict_from_sentences(n1_context_train, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a047453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bigrams for train\n",
    "n2_context_train, n2_context_test = train_test_split_and_tokenize(context, n=2, verbose=True)\n",
    "print(\"Getting bigrams...\")\n",
    "bigrams = get_ngrams_dict_from_sentences(n2_context_train, 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine trigrams for train\n",
    "n3_context_train, n3_context_test = train_test_split_and_tokenize(context, n=3, verbose=True)\n",
    "print(\"Getting trigrams...\")\n",
    "trigrams = get_ngrams_dict_from_sentences(n3_context_train, 3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine 4-grams for train\n",
    "n4_context_train, n4_context_test = train_test_split_and_tokenize(context, n=4, verbose=True)\n",
    "print(\"Getting fourgrams...\")\n",
    "fourgrams = get_ngrams_dict_from_sentences(n4_context_train, 4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115791",
   "metadata": {},
   "source": [
    "## Verifying correctness of NGramModel\n",
    "First we regenerate probabilities from example in SLP book, then we verify our model against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_series = pl.Series([\"I am Sam\", \"Sam I am\", \"I do not like green eggs and ham\"])\n",
    "cased_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "cased_tokenizer.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "ngram_ready_tokens = [tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=2) for seq in mock_series]\n",
    "mock_data = ngram_ready_tokens # work for both uni- and bigrams\n",
    "mock_nm1grams = get_ngrams_dict_from_sentences(mock_data, 1, verbose=True)\n",
    "mock_ngrams = get_ngrams_dict_from_sentences(mock_data, 2, verbose=True)\n",
    "for row in mock_data:\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "NGram = tuple[str, ...]\n",
    "Tokens: TypeAlias = list[str]\n",
    "TokenizedSentences: TypeAlias = list[Tokens]\n",
    "\n",
    "class NGramLM:\n",
    "    \"\"\"Class to represent an N-gram language model.\n",
    "    The model takes as input the n-grams and (n-1)-grams count dictionaries,\n",
    "    and computes the conditional probabilities of the n-grams given the (n-1)-grams. \n",
    "    \"\"\"\n",
    "    def __init__(self, nm1grams: NGramsDict, ngrams: NGramsDict):\n",
    "        self.ngrams = ngrams\n",
    "        self.nm1grams = nm1grams\n",
    "        self.n = len(list(ngrams.keys())[0])\n",
    "        self.probabilities = {key: 0.0 for key in self.ngrams.keys()}\n",
    "        self._calc_word_probabilities()\n",
    "\n",
    "    def _calc_word_probabilities(self) -> None:\n",
    "        \"\"\"For word (token) in vocabulary, estimate probabilities by counts\"\"\"\n",
    "        for ngram in self.ngrams.keys():\n",
    "            word = ngram[-1]\n",
    "            prefix = ngram[:-1]\n",
    "            self.probabilities[ngram] = self.ngrams[ngram] / self.nm1grams[prefix]\n",
    "    \n",
    "    def get_sentence_probability(self, sentence: list[NGram]) -> float:\n",
    "        \"\"\"Get the probability of a sentence (list of tokens) under this model.\"\"\"\n",
    "        log_prob = 0.0\n",
    "        for ngram in sentence:\n",
    "            pass\n",
    "        \n",
    "        return math.exp(log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b56fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGramLM(mock_nm1grams, mock_ngrams)\n",
    "for key, item in bigram_model.probabilities.items():\n",
    "    print(f\"{key}: {item:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_sentence = \"I am Sam\"\n",
    "tokenize(mock_sentence, tokenizer=cased_tokenizer.tokenize, n=2)\n",
    "# Should ignore \"b\" as it is OOV and return 2/3 * 2/3 * 1/2 * 1/2 = 1/9 = 0.1111\n",
    "#print(bigram_model.get_sentence_probability(mock_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
