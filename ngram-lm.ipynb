{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643ea4c",
   "metadata": {},
   "source": [
    "# Week 37: LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f37f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from typing import TypeAlias, cast\n",
    "from ngrams.utils import (\n",
    "    get_model_ready_data,\n",
    "    get_ngrams_dict_from_sentences,\n",
    "    _pad_and_tokenize,\n",
    "    my_tokenize,\n",
    "    NGram,\n",
    "    NGramsDict,\n",
    "    ModelReadyData,\n",
    ")\n",
    "import nltk\n",
    "from ngrams.models import NGramLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "assert isinstance(dataset, DatasetDict), \"Loaded dataset is not a DatasetDict\"\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "assert isinstance(df_train, pl.DataFrame), \"Training set is not a Polars DataFrame\"\n",
    "assert isinstance(df_val, pl.DataFrame), \"Validation set is not a Polars DataFrame\"\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_ar_train = df_train.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ar_val = df_val.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\")\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_ko_train = df_train.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_ko_val = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_arkote_train = df_train.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))\n",
    "df_arkote_val = df_val.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))\n",
    "df_arkote = pl.concat([df_arkote_train, df_arkote_val])\n",
    "\n",
    "# Make a dict\n",
    "data = {\n",
    "    \"arabic\": {\"train\": df_ar_train, \"val\": df_ar_val},\n",
    "    \"telegu\": {\"train\": df_te_train, \"val\": df_te_val},\n",
    "    \"korean\": {\"train\": df_ko_train, \"val\": df_ko_val},\n",
    "    \"full\": {\"train\": df_train, \"val\": df_val},\n",
    "}\n",
    "df_ar_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf7c8",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa4a02",
   "metadata": {},
   "source": [
    "### Examine the corpus stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average (train) context length\n",
    "context = df_arkote[\"context\"]\n",
    "avg_len = sum(len(c) for c in context) / len(context)\n",
    "print(f\"Average context length: {avg_len:.2f} characters\")\n",
    "print(f\"Number of sequences (rows) in context: {len(context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english corpus\n",
    "context_corpus = df_arkote[\"context\"].to_list()\n",
    "# Get number of unique space seperated words (not tokens)\n",
    "context_vocab = set(\" \".join(context_corpus).split())\n",
    "number_of_unique_words = len(context_vocab)\n",
    "print(f\"Number of unique (space seperated) words in context: {number_of_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898097cd",
   "metadata": {},
   "source": [
    "### Tokenize corpus\n",
    "Here we use Multilingual BERT tokenizer. We use identical tokenizer for comparing perplexity.\n",
    "Each string entrance may be several sentences, but for simplicity we are gonna treat each one as a single sequence, and use the inherent start- and end-of-sentence markers from mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multilingual bert tokenizer\n",
    "mbert = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "mbert.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "\n",
    "# Example tokenization\n",
    "sample_content_tokens = _pad_and_tokenize(df_arkote[\"context\"][10], n=5)\n",
    "print(f\"Sample content tokens (n=5):\")\n",
    "\" | \".join(sample_content_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de37518",
   "metadata": {},
   "source": [
    "## N-Gram LM\n",
    "First we explore some statistics of $n$, to pick the size we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eea4a",
   "metadata": {},
   "source": [
    "### Examine NGramDicts for context series for N={1, 2, 3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine unigrams for train\n",
    "n1_context_train = my_tokenize(context, n=1)\n",
    "print(\"Getting unigrams...\")\n",
    "unigrams = get_ngrams_dict_from_sentences(n1_context_train, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a047453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bigrams for train\n",
    "n2_context_train = my_tokenize(context, n=2)\n",
    "print(\"Getting bigrams...\")\n",
    "bigrams = get_ngrams_dict_from_sentences(n2_context_train, 2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine trigrams for train\n",
    "n3_context_train = my_tokenize(context, n=3)\n",
    "print(\"Getting trigrams...\")\n",
    "trigrams = get_ngrams_dict_from_sentences(n3_context_train, 3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine 4-grams for train\n",
    "n4_context_train = my_tokenize(context, n=4)\n",
    "print(\"Getting fourgrams...\")\n",
    "fourgrams = get_ngrams_dict_from_sentences(n4_context_train, 4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68133a36",
   "metadata": {},
   "source": [
    "### NGramModel from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115791",
   "metadata": {},
   "source": [
    "## Verifying correctness of NGramModel\n",
    "First we regenerate probabilities from example in SLP book, then we verify our model against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_series = pl.Series([\"I am Sam\", \"Sam I am\", \"I do not like green eggs and ham\"])\n",
    "cased_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "cased_tokenizer.add_tokens([\"<s>\", \"</s>\"])  # Add start and end tokens\n",
    "ngram_ready_tokens = [_pad_and_tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=2) for seq in mock_series]\n",
    "mock_data = ngram_ready_tokens # work for both uni- and bigrams\n",
    "mock_nm1grams = get_ngrams_dict_from_sentences(mock_data, 1, verbose=True)\n",
    "mock_ngrams = get_ngrams_dict_from_sentences(mock_data, 2, verbose=True)\n",
    "for row in mock_data:\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b56fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGramLM(mock_nm1grams, mock_ngrams, smoothing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_sentence = \"I am Sam\"\n",
    "mock_sentence = _pad_and_tokenize(mock_sentence, tokenizer=cased_tokenizer.tokenize, n=2)\n",
    "bigram_mock_sentence = list(nltk.ngrams(mock_sentence, 2))\n",
    "# Should ignore \"b\" as it is OOV and return 2/3 * 2/3 * 1/2 * 1/2 = 1/9 = 0.1111\n",
    "print(bigram_model.get_sentence_probability(bigram_mock_sentence, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f790d",
   "metadata": {},
   "source": [
    "Count num words via ngrams (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get N for bi, tri and fourgrams\n",
    "#for n in [2, 3, 4]:\n",
    "#    ngram_ready_tokens = [tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=n) for seq in mock_series]\n",
    "#    ngram_mock_sentences = [list(nltk.ngrams(seq, n)) for seq in ngram_ready_tokens]\n",
    "#    print(f\"Sample ngram: {ngram_mock_sentences[0] if ngram_mock_sentences else None}\")\n",
    "#    # Should match num tokens excluding <s>\n",
    "#    print(f\"Len of sample ngram (sanity check): {len(ngram_mock_sentences[0])}\")\n",
    "#    N = sum([len(sentence) for sentence in ngram_mock_sentences]) - len(ngram_mock_sentences)\n",
    "#    print(f\"N for {n}-grams: {N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity of mock example\n",
    "ngram_ready_tokens = [_pad_and_tokenize(seq, tokenizer=cased_tokenizer.tokenize, n=2) for seq in mock_series]\n",
    "bigram_mock_sentences = [list(nltk.ngrams(seq, 2)) for seq in ngram_ready_tokens]\n",
    "# Mock sentence [\"<s>\", \"I\", \"am\", \"Sam\", \"</s>\"], seq prob = 1/9  (not counting <s>)\n",
    "# thus perplexity = (1/9)^(-1/4) = sqrt(3) approx 1.732\n",
    "print(bigram_model.get_perplexity([bigram_mock_sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe3a93",
   "metadata": {},
   "source": [
    "## Get Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [2]:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Calculating {n}-gram LM perplexity for each of 4 corpora...\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    for corpus in [\"arabic\", \"telegu\", \"korean\", \"full\"]:\n",
    "        col_name = \"question\" if not \"full\" else \"context\"\n",
    "        train, val = data[corpus][\"train\"][col_name], data[corpus][\"val\"][col_name]\n",
    "        unigram, bigram, test = get_model_ready_data(train, val, n)\n",
    "        n_gram_lm = NGramLM(unigram, bigram, smoothing=\"laplace\")\n",
    "        model_perplexity = n_gram_lm.get_perplexity(test)\n",
    "        print(f\"{corpus.capitalize()} {n}-gram LM perplexity: {model_perplexity:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
