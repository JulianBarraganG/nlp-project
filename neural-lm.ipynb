{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05d5f30",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d842b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from nlm.models import BiLSTMLanguageModel\n",
    "from nlm.train_utils import train_lm as train\n",
    "from nlm.probs import sentence_log_probability, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c0cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = pl.from_pandas(dataset[\"train\"].to_pandas())\n",
    "df_val = pl.from_pandas(dataset[\"validation\"].to_pandas())\n",
    "\n",
    "df_ar_train = df_train.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ko_train = df_train.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\")\n",
    "df_arkote_train = df_train.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))\n",
    "\n",
    "df_ar_val = df_val.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ko_val = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_arkote_val = df_val.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0a04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mBERT tokenizer\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "mbert_model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "pretrained_embeddings = mbert_model.get_input_embeddings().weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de32f3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b991ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_generator(train_dataset: list, val_dataset: list, tokenizer, device, batch_size: int = 8) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Generate DataLoader objects for training and validation datasets for use in PyTorch models.\n",
    "    \"\"\"\n",
    "    train_tokens = tokenizer(\n",
    "        train_dataset,\n",
    "        truncation=True,\n",
    "        max_length=65,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    val_tokens = tokenizer(\n",
    "        val_dataset,\n",
    "        truncation=True,\n",
    "        max_length=65,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    train_input_ids = train_tokens['input_ids']\n",
    "    train_input_lens = train_tokens['attention_mask'].sum(dim=1)\n",
    "    val_input_ids = val_tokens['input_ids']\n",
    "    val_input_lens = val_tokens['attention_mask'].sum(dim=1)\n",
    "\n",
    "    # Shift input_ids for targets\n",
    "    train_targets = train_input_ids.clone()\n",
    "    train_targets[:, :-1] = train_input_ids[:, 1:]\n",
    "    train_targets[:, -1] = tokenizer.pad_token_id\n",
    "    val_targets = val_input_ids.clone()\n",
    "    val_targets[:, :-1] = val_input_ids[:, 1:]\n",
    "    val_targets[:, -1] = tokenizer.pad_token_id\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        train_input_ids, train_input_lens, train_targets\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        val_input_ids, val_input_lens, val_targets\n",
    "    )\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dl, val_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976116c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model_loader(train_dataset: list, val_dataset: list, device, model_cache_path: str, epochs: int, model_lstm_dim: int = 100) -> tuple[BiLSTMLanguageModel, float, float]:\n",
    "    model = BiLSTMLanguageModel(\n",
    "        pretrained_embeddings=torch.FloatTensor(pretrained_embeddings).to(device),\n",
    "        lstm_dim=model_lstm_dim\n",
    "    ).to(device)\n",
    "\n",
    "    if os.path.exists(model_cache_path):\n",
    "        print(\"Loading cached model from\", model_cache_path)\n",
    "        model.load_state_dict(torch.load(model_cache_path))\n",
    "    else:\n",
    "        print(\"No cached model found. Training a new model.\")\n",
    "        train_dl, val_dl = dataloader_generator(train_dataset, val_dataset, mbert_tokenizer, device)\n",
    "        losses, best_acc = train(model, train_dl, val_dl, torch.optim.Adam(model.parameters(), lr=1e-3), n_epochs=epochs, device=device, save_path=model_cache_path)\n",
    "        print('Training complete. Best validation accuracy:', best_acc)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c122a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arn/Desktop/nlp-project/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached model found. Training a new model.\n",
      "Validation accuracy: 0.8326227988878592, train loss: 5.824188381433487\n",
      "Validation accuracy: 0.8517145505097312, train loss: 4.206540562212467\n",
      "Validation accuracy: 0.09456904541241891, train loss: 3.645081780105829\n",
      "Training complete. Best validation accuracy: 0.8517145505097312\n"
     ]
    }
   ],
   "source": [
    "# Arabic dataset\n",
    "arabic_model_path = \"cached_data/bilstm_lm_arabic\"\n",
    "df_arabic_train_questions = df_ar_train[\"question\"].to_list()\n",
    "df_arabic_val_questions = df_ar_val[\"question\"].to_list()\n",
    "\n",
    "arabic_model = ml_model_loader(df_arabic_train_questions, df_arabic_val_questions, device, arabic_model_path, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86629806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached model found. Training a new model.\n",
      "Validation accuracy: 0.7699654278305964, train loss: 5.314905911782394\n",
      "Validation accuracy: 0.808124459809853, train loss: 4.072393501552418\n",
      "Validation accuracy: 0.8300777873811581, train loss: 3.41679582186658\n",
      "Training complete. Best validation accuracy: 0.8300777873811581\n"
     ]
    }
   ],
   "source": [
    "# Korean dataset\n",
    "korean_model_path = \"cached_data/bilstm_lm_korean\"\n",
    "df_korean_train_questions = df_ko_train[\"question\"].to_list()\n",
    "df_korean_val_questions = df_ko_val[\"question\"].to_list()\n",
    "\n",
    "korean_model = ml_model_loader(df_korean_train_questions, df_korean_val_questions, device, korean_model_path, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9203e534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached model found. Training a new model.\n",
      "Validation accuracy: 0.7860576923076923, train loss: 5.716956051658181\n",
      "Validation accuracy: 0.7921474358974359, train loss: 4.177906838585349\n",
      "Validation accuracy: 0.8014423076923077, train loss: 3.8945506348329433\n",
      "Training complete. Best validation accuracy: 0.8014423076923077\n"
     ]
    }
   ],
   "source": [
    "# Telughu dataset\n",
    "telugu_model_path = \"cached_data/bilstm_lm_telugu\"\n",
    "df_telugu_train_questions = df_te_train[\"question\"].to_list()\n",
    "df_telugu_val_questions = df_te_val[\"question\"].to_list()\n",
    "\n",
    "telugu_model = ml_model_loader(df_telugu_train_questions, df_telugu_val_questions, device, telugu_model_path, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e0b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cached model found. Training a new model.\n",
      "Validation accuracy: 0.18530802530802531, train loss: 7.048145920339257\n",
      "Validation accuracy: 0.42195138195138193, train loss: 4.943429743400728\n",
      "Validation accuracy: 0.5213719613719614, train loss: 3.6819923197982285\n",
      "Training complete. Best validation accuracy: 0.5213719613719614\n"
     ]
    }
   ],
   "source": [
    "# Context dataset\n",
    "context_model_path = \"cached_data/bilstm_lm_context\"\n",
    "df_context_train = df_arkote_train[\"context\"].to_list()\n",
    "df_context_val = df_arkote_val[\"context\"].to_list()\n",
    "\n",
    "context_model = ml_model_loader(df_context_train, df_context_val, device, context_model_path, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f774d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.18937026290223, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_stc = \"I am Sam\"\n",
    "sentence_log_probability(context_model, device, mbert_tokenizer, tst_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327a6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the Korean training text: 1.000318561656128 and validation text: 1.0028498705226396\n",
      "Perplexity of the Telugu training text: 1.000870833682479 and validation text: 1.0034615842719716\n",
      "Perplexity of the Arabic training text: 1.0002382131330585 and validation text: 1.0026154111887515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the Context training text: 1.0000202527721265 and validation text: 1.000136372225572\n"
     ]
    }
   ],
   "source": [
    "perplex_korean_train = perplexity(\n",
    "    korean_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_korean_train_questions\n",
    ")\n",
    "perplex_korean_val = perplexity(\n",
    "    korean_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_korean_val_questions\n",
    ")\n",
    "print(f\"Perplexity of the Korean training text: {perplex_korean_train} and validation text: {perplex_korean_val}\")\n",
    "\n",
    "perplex_telugu_train = perplexity(\n",
    "    telugu_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_telugu_train_questions\n",
    ")\n",
    "perplex_telugu_val = perplexity(\n",
    "    telugu_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_telugu_val_questions\n",
    ")\n",
    "print(f\"Perplexity of the Telugu training text: {perplex_telugu_train} and validation text: {perplex_telugu_val}\")\n",
    "\n",
    "perplex_arabic_train = perplexity(\n",
    "    arabic_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_arabic_train_questions\n",
    ")\n",
    "perplex_arabic_val = perplexity(\n",
    "    arabic_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_arabic_val_questions\n",
    ")\n",
    "print(f\"Perplexity of the Arabic training text: {perplex_arabic_train} and validation text: {perplex_arabic_val}\")\n",
    "\n",
    "perplex_context_train = perplexity(\n",
    "    context_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_context_train\n",
    ")\n",
    "perplex_context_val = perplexity(\n",
    "    context_model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    df_context_val\n",
    ")\n",
    "print(f\"Perplexity of the Context training text: {perplex_context_train} and validation text: {perplex_context_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
