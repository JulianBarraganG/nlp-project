{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05d5f30",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d842b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arkote = pl.concat([\n",
    "    pl.read_parquet(ARB_CACHE),\n",
    "    pl.read_parquet(KOR_CACHE),\n",
    "    pl.read_parquet(TELU_CACHE)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "nllb_model = AutoModel.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "pretrained_embeddings = nllb_model.get_input_embeddings().weight.data\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM Language Model for sentence probability\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings: torch.tensor, \n",
    "                 lstm_dim: int, \n",
    "                 dropout_prob: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        \"\"\"\n",
    "        # First thing is to call the superclass initializer\n",
    "        super().__init__()\n",
    "\n",
    "        # Get vocab size and embedding dimension from the pretrained embeddings\n",
    "        vocab_size = pretrained_embeddings.shape[0] # Size of the vocabulary\n",
    "        embed_dim = pretrained_embeddings.shape[1] # Dimensionality of the embeddings\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=vocab_size - 1),\n",
    "            'bilstm': nn.LSTM(embed_dim, lstm_dim, 1, batch_first=True, dropout=dropout_prob, bidirectional=True),\n",
    "            'lm_head': nn.Linear(2 * lstm_dim, vocab_size)\n",
    "        })\n",
    "        self.n_classes = vocab_size\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['lm_head'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :return: logits\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds, \n",
    "            input_lens.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, _ = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        ff_in = self.dropout(lstm_out)\n",
    "\n",
    "        logits = self.model['lm_head'](ff_in)\n",
    "\n",
    "        return logits  # (batch, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 100\n",
    "\n",
    "model = BiLSTMLanguageModel(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim\n",
    "  ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303019df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl):\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            # Move batch to device\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs, input_lens, targets = batch\n",
    "            # Get logits from the model\n",
    "            logits = model(inputs, input_lens)\n",
    "            # Get predictions\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            # Create mask to ignore padding in accuracy calculation\n",
    "            mask = (targets != logits.size(-1)-1)\n",
    "            # Calculate number of correct predictions\n",
    "            correct += ((preds == targets) & mask).sum().item()\n",
    "            # Calculate total number of tokens\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb57c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, \n",
    "        train_dl: torch.utils.data.DataLoader, \n",
    "        valid_dl: torch.utils.data.DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        n_epochs: int, \n",
    "        device: torch.device,\n",
    "        patience: int = 10\n",
    "    ):\n",
    "    '''\n",
    "    Train a language model with early stopping on validation accuracy.\n",
    "    '''\n",
    "    losses = []\n",
    "    best_acc = 0.0\n",
    "    pcounter = 0\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "\n",
    "        loss_epoch = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs, input_lens, targets = batch\n",
    "            logits = model(inputs, input_lens)\n",
    "            # Mask out padding tokens using input_lens\n",
    "            batch_size, seq_len, vocab_size = logits.size()\n",
    "            mask = torch.arange(seq_len)[None, :].to(device) < input_lens[:, None]\n",
    "            logits_flat = logits[mask]\n",
    "            targets_flat = targets[mask]\n",
    "            loss = nn.functional.cross_entropy(logits_flat, targets_flat, ignore_index=logits.size(-1)-1)\n",
    "            loss_epoch.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = np.mean(loss_epoch)\n",
    "        losses.append(avg_train_loss)\n",
    "\n",
    "        acc = evaluate(model, valid_dl)\n",
    "        print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "        # Keep track of the best model based on the accuracy\n",
    "        if acc > best_acc:\n",
    "            torch.save(model.state_dict(), 'best_model')\n",
    "            best_acc = acc\n",
    "            pcounter = 0\n",
    "        else:\n",
    "            pcounter += 1\n",
    "            if pcounter == patience:\n",
    "                break\n",
    "    model.load_state_dict(torch.load('best_model'))\n",
    "    return losses, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b991ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine context and translation\n",
    "texts = df_arkote['context'].to_list() + df_arkote['translation'].to_list()\n",
    "texts_small = texts[:1000]\n",
    "\n",
    "# Batch tokenize\n",
    "tokens = nllb_tokenizer(\n",
    "    texts_small,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    " )\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "input_lens = attention_mask.sum(dim=1)\n",
    "\n",
    "# Shift input_ids for targets\n",
    "targets = input_ids.clone()\n",
    "targets[:, :-1] = input_ids[:, 1:]\n",
    "targets[:, -1] = nllb_tokenizer.pad_token_id\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(input_ids.size(0)), test_size=0.2, random_state=42\n",
    " )\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids[train_idx], input_lens[train_idx], targets[train_idx]\n",
    " )\n",
    "val_dataset = TensorDataset(\n",
    "    input_ids[val_idx], input_lens[val_idx], targets[val_idx]\n",
    " )\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "losses, best_acc = train(model, train_dl, val_dl, torch.optim.Adam(model.parameters(), lr=1e-3), n_epochs=5, device=device)\n",
    "print('Training complete. Best validation accuracy:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_probability(model, sentence_ids, sentence_len):\n",
    "    '''\n",
    "    Compute log-probability of a sentence under the language model.\n",
    "    sentence_ids: torch.LongTensor of shape (1, seq_len)\n",
    "    sentence_len: torch.LongTensor of shape (1,)\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(sentence_ids, sentence_len)\n",
    "        probs = torch.log_softmax(logits, dim=-1)\n",
    "        # Shift sentence_ids for next-token prediction\n",
    "        target = sentence_ids[:, 1:]\n",
    "        input_probs = probs[:, :-1, :]\n",
    "        # Gather log-probs for actual next tokens\n",
    "        sentence_token_log_probs = input_probs.gather(2, target.unsqueeze(-1)).squeeze(-1)\n",
    "        # Mask for actual length\n",
    "        mask = torch.arange(sentence_ids.size(1)-1)[None, :] < (sentence_len-1)[:, None]\n",
    "        total_log_prob = sentence_token_log_probs[mask].sum().item()\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "example_sentence = \"This is a test sentence.\"\n",
    "\n",
    "sentence_ids = nllb_tokenizer(example_sentence, return_tensors='pt')['input_ids']\n",
    "sentence_len = torch.LongTensor([len(nllb_tokenizer(example_sentence)['input_ids'])])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(sentence_ids, sentence_len)\n",
    "    probs = torch.log_softmax(logits, dim=-1)\n",
    "    print(probs.shape)\n",
    "    print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokenizer,sentence_list):\n",
    "    '''\n",
    "    Compute perplexity of a sentence under the language model.\n",
    "    sentence_ids: torch.LongTensor of shape (1, seq_len)\n",
    "    sentence_len: torch.LongTensor of shape (1,)\n",
    "    '''\n",
    "    props = 0\n",
    "    tokens = 0\n",
    "    for sentence in sentence_list:\n",
    "        sentence_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
    "        sentence_len = torch.LongTensor([len(tokenizer(sentence)['input_ids'])])\n",
    "        log_prob = sentence_log_probability(model, sentence_ids, sentence_len)\n",
    "        props += log_prob\n",
    "        tokens += (sentence_len - 1).item()\n",
    "    props = math.exp(props)\n",
    "    return math.pow(props, 1 / tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of sentence_log_probability\n",
    "example_sentence = \"This is a test sentence.\"\n",
    "log_prob = sentence_log_probability(\n",
    "    model, \n",
    "    nllb_tokenizer(example_sentence, return_tensors='pt')['input_ids'], \n",
    "    torch.LongTensor([len(nllb_tokenizer(example_sentence)['input_ids'])])\n",
    ")\n",
    "print(f\"Log probability of the sentence: {log_prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = \" \".join(small_texts)\n",
    "perplex = perplexity(\n",
    "    model, \n",
    "    \n",
    ")\n",
    "print(f\"Perplexity of the sentence: {perplex}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
