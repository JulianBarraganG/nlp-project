{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05d5f30",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d842b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from data.const import ARB_CACHE, KOR_CACHE, TELU_CACHE\n",
    "from nlm.models import BiLSTMLanguageModel\n",
    "from nlm.train_utils import train\n",
    "from nlm.probs import sentence_log_probability, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arkote = pl.concat([\n",
    "    pl.read_parquet(ARB_CACHE),\n",
    "    pl.read_parquet(KOR_CACHE),\n",
    "    pl.read_parquet(TELU_CACHE)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "mbert_model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "pretrained_embeddings = mbert_model.get_input_embeddings().weight.data\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dim = 100\n",
    "\n",
    "model = BiLSTMLanguageModel(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim\n",
    "  ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b991ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Factor out data loading into DataLoader\n",
    "# Combine context and translation\n",
    "context = df_arkote[\"context\"].to_list()\n",
    "\n",
    "# Batch tokenize\n",
    "tokens = mbert_tokenizer(\n",
    "    context,\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    " ).to(device)\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']\n",
    "input_lens = attention_mask.sum(dim=1)\n",
    "\n",
    "# Shift input_ids for targets\n",
    "targets = input_ids.clone()\n",
    "targets[:, :-1] = input_ids[:, 1:]\n",
    "targets[:, -1] = mbert_tokenizer.pad_token_id\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(input_ids.size(0)), test_size=0.2, random_state=42\n",
    " )\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids[train_idx], input_lens[train_idx], targets[train_idx]\n",
    " )\n",
    "val_dataset = TensorDataset(\n",
    "    input_ids[val_idx], input_lens[val_idx], targets[val_idx]\n",
    " )\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"cached_models/bilstm_lm\"):\n",
    "    model.load_state_dict(torch.load(\"cached_models/bilstm_lm\"))\n",
    "losses, best_acc = train(model, train_dl, val_dl, torch.optim.Adam(model.parameters(), lr=1e-3), n_epochs=5, device=device)\n",
    "print('Training complete. Best validation accuracy:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f774d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_stc = \"I am Sam\"\n",
    "sentence_log_probability(model, device, mbert_tokenizer, tst_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplex = perplexity(\n",
    "    model, \n",
    "    device,\n",
    "    mbert_tokenizer,\n",
    "    context,\n",
    ")\n",
    "print(f\"Perplexity of the sentence: {perplex}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
