{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808e1f1c",
   "metadata": {},
   "source": [
    "# Week 40 - Classifying Span containing Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Huggingface imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c82cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_ar_train = df_train.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ar_val = df_val.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\")\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_ko_train = df_train.filter((pl.col(\"lang\") == \"ko\") & (pl.col(\"answerable\") == True))\n",
    "df_ko_val = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "\n",
    "# Make a dict\n",
    "data = {\n",
    "    \"arabic\": {\"train\": df_ar_train, \"val\": df_ar_val},\n",
    "    \"telegu\": {\"train\": df_te_train, \"val\": df_te_val},\n",
    "    \"korean\": {\"train\": df_ko_train, \"val\": df_ko_val},\n",
    "}\n",
    "assert df_ko_train.height == sum(df_ko_train[\"answerable\"]), \"All answers should be answerable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ko_train[\"answer\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66491ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_cont = df_ko_train[\"context\"][1]\n",
    "tst_answer = df_ko_train[\"answer\"][1]\n",
    "print(\"CONTEXT: \", tst_cont)\n",
    "print(\"ANSWER: \", tst_answer)\n",
    "\n",
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)\n",
    "print(\"CONTEXT TOKEN: \", mbert_tokenizer(tst_cont, return_offsets_mapping=True)[\"input_ids\"])\n",
    "print(\"ANSWER TOKEN: \", mbert_tokenizer(tst_answer, return_offsets_mapping=True)[\"input_ids\"])\n",
    "print(\"ANSWER OFFSETS: \", mbert_tokenizer(tst_answer, return_offsets_mapping=True)[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2bio = { 0: \"O\", 1: \"B-ANS\", 2: \"I-ANS\"}\n",
    "bio2num = {\"O\": 0, \"B-ANS\": 1, \"I-ANS\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_labeler(\n",
    "    context: str, \n",
    "    answer_start: int, \n",
    "    answer_text: str,\n",
    ") -> list:\n",
    "    \"\"\"Enhanced version with validation.\"\"\"\n",
    "    \n",
    "    if answer_start == -1:  # Unanswerable\n",
    "        encoding = mbert_tokenizer(context, return_offsets_mapping=True)\n",
    "        return [0] * len(encoding[\"input_ids\"])\n",
    "    \n",
    "    answer_end = answer_start + len(answer_text) # if answer_text else answer_start + 1\n",
    "    \n",
    "    encoding = mbert_tokenizer(\n",
    "        context,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    tokens = encoding[\"input_ids\"]\n",
    "    offset_mapping = encoding[\"offset_mapping\"]\n",
    "    labels = np.zeros(len(tokens), dtype=np.int8)\n",
    "    \n",
    "    answer_token_indices = []\n",
    "    for idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "        if token_start == 0 and token_end == 0:\n",
    "            continue\n",
    "        if token_start < answer_end and token_end > answer_start:\n",
    "            answer_token_indices.append(idx)\n",
    "\n",
    "\n",
    "    if answer_token_indices:\n",
    "        answer_tokens_ids = [tokens[i] for i in answer_token_indices]\n",
    "        # print(f\"Answer tokens: {(answer_tokens_ids)}\")\n",
    "        for i in range(len(tokens) - len(answer_tokens_ids) + 1): \n",
    "            if tokens[i:i+len(answer_tokens_ids)] == answer_tokens_ids:\n",
    "                # print(f\"Match found at token indices: {list(range(i, i+len(answer_tokens_ids)))}\")\n",
    "                # print(f\"Corresponding context: '{context[max(0, offset_mapping[i][0] - 20) : offset_mapping[i+len(answer_tokens_ids)-1][1] + 20]}'\")\n",
    "                labels[i] = 1  # B-ANS\n",
    "                for j in range(i+1, i+len(answer_tokens_ids)):\n",
    "                    labels[j] = 2  # I-ANS\n",
    "        \n",
    "        #labels[answer_token_indices[0]] = 1\n",
    "        #for idx in answer_token_indices[1:]:\n",
    "        #    labels[idx] = 2\n",
    "\n",
    "    elif answer_text:  # Validation mode\n",
    "        print(f\"WARNING: No tokens found for answer '{answer_text}' at position {answer_start}\")\n",
    "        print(f\"Context: {context[max(0, answer_start-20):answer_start+len(answer_text)+20]}\")\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ko_train = df_ko_train.with_columns(\n",
    "    pl.struct([\"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: sequence_labeler(x[\"context\"], x[\"answer_start\"], x[\"answer\"]), return_dtype=pl.List(pl.Int8)\n",
    "    ).alias(\"labels\")\n",
    ")\n",
    "df_ko_val = df_ko_val.with_columns(\n",
    "    pl.struct([\"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: sequence_labeler(x[\"context\"], x[\"answer_start\"], x[\"answer\"]), return_dtype=pl.List(pl.Int8)\n",
    "    ).alias(\"labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pl.DataFrame) -> Dataset:\n",
    "    # Convert Polars to dict format for HF datasets\n",
    "    data_dict = {\n",
    "        \"question\": df[\"question\"].to_list(),\n",
    "        \"context\": df[\"context\"].to_list(),\n",
    "        \"label\": df[\"labels\"]\n",
    "    }\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def tokenize_function(examples: Dataset, tokenizer: AutoTokenizer) -> Dataset:\n",
    "    # Tokenize with question and content separated by [SEP]\n",
    "    # [CLS] is added automatically\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ) # type: ignore\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = prepare_data(df_ko_train)\n",
    "val_dataset = prepare_data(df_ko_val)\n",
    "tokenized_train = train_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n",
    "tokenized_val = val_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from transformers import Seq2SeqTrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(mbert_checkpoint, num_labels=3)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbert-iob\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=mbert_tokenizer,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "def predict(\n",
    "    question: pl.Series,\n",
    "    context: pl.Series,\n",
    "    model: BertForTokenClassification,\n",
    "    tokenizer: AutoTokenizer\n",
    "):\n",
    "    \"\"\"Get model prediction for a single example\"\"\"\n",
    "\n",
    "    print(question)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        context, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ) # type: ignore\n",
    "\n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        model = model.cuda() # type: ignore\n",
    "    else:\n",
    "        inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "        model = model.cpu() # type: ignore\n",
    "    \n",
    "    model.eval() # type: ignore\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs) # type: ignore\n",
    "        logits = outputs.logits\n",
    "        #probs = torch.softmax(logits, dim=1)\n",
    "        #prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "        probs = torch.softmax(logits, dim=2)  # Softmax over the last dimension (num_labels)\n",
    "        prediction = torch.argmax(logits, dim=2).squeeze().tolist()  #\n",
    "\n",
    "    return prediction\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(mbert_checkpoint, num_labels=3)\n",
    "\n",
    "predict(\n",
    "    df_ko_val[\"question\"][:10].to_list(), \n",
    "    df_ko_val[\"context\"][:10].to_list(), \n",
    "    model,\n",
    "    mbert_tokenizer,\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
