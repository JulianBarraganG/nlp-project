{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808e1f1c",
   "metadata": {},
   "source": [
    "# Week 40 - Classifying Span containing Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "# Huggingface imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForTokenClassification,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# Import own modules\n",
    "from bert_utils import (\n",
    "    bio_sequence_labeler,\n",
    "    get_results,\n",
    "    display_results,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c82cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_train = df_train.filter(pl.col(\"lang\").is_in([\"ar\", \"te\", \"ko\"]))\n",
    "df_val = df_val.filter(pl.col(\"lang\").is_in([\"ar\", \"te\", \"ko\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66491ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9877ce",
   "metadata": {},
   "source": [
    "### Create label columns\n",
    "Note that we cut off at 512 tokens. Two instances will have their answer after 512. We do this for simplicity, since the model has 512 token max, and we expect the model to generalize well without these two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fda212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BIO label column for train and val\n",
    "df_train = df_train.with_columns(\n",
    "    pl.struct([\"question\", \"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: bio_sequence_labeler(\n",
    "            x[\"answer_start\"],\n",
    "            x[\"answer\"],\n",
    "            x[\"question\"],\n",
    "            x[\"context\"],\n",
    "            mbert_tokenizer,\n",
    "        ),\n",
    "        return_dtype=pl.List(pl.Int8)\n",
    "    ).alias(\"labels\")\n",
    ")\n",
    "df_val = df_val.with_columns(\n",
    "    pl.struct([\"question\", \"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: bio_sequence_labeler(\n",
    "            x[\"answer_start\"],\n",
    "            x[\"answer\"],\n",
    "            x[\"question\"],\n",
    "            x[\"context\"],\n",
    "            mbert_tokenizer,\n",
    "        ),\n",
    "        return_dtype=pl.List(pl.Int8)\n",
    "    ).alias(\"labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d966b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pl.DataFrame) -> Dataset:\n",
    "    # Convert Polars to dict format for HF datasets\n",
    "    data_dict = {\n",
    "        \"question\": df[\"question\"].to_list(),\n",
    "        \"context\": df[\"context\"].to_list(),\n",
    "        \"labels\": df[\"labels\"].to_list(),\n",
    "    }\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "def tokenize_function(examples: Dataset, tokenizer: AutoTokenizer) -> Dataset:\n",
    "    # Tokenize with question and content separated by [SEP]\n",
    "    # [CLS] is added automatically\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ) # type: ignore\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = prepare_data(df_train)\n",
    "val_dataset = prepare_data(df_val)\n",
    "tokenized_train = train_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n",
    "tokenized_val = val_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab1cbf",
   "metadata": {},
   "source": [
    "## Check model performance before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d290bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(mbert_checkpoint, num_labels=3)\n",
    "val_set = df_val.sample(50, shuffle=True) # Just to see poor performance quickly\n",
    "display_results(*get_results(model, val_set, mbert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weird memory issue fix\n",
    "#model = None # before GC\n",
    "#gc.collect()\n",
    "#with torch.no_grad():\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell\n",
    "output_path = \"./mbert-iob\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    # Memory efficient params\n",
    "    fp16=False,\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    output_dir=output_path,\n",
    "    overwrite_output_dir = True,\n",
    "    learning_rate=2e-5,\n",
    "\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01, # L2 regularization term\n",
    "    generation_max_length=512,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    save_strategy = \"best\",\n",
    "    load_best_model_at_end = True,\n",
    "    \n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    log_level=\"info\",\n",
    "    report_to=[],\n",
    "    logging_dir=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b108c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ideally these are global vars up top.\n",
    "model_path = os.path.join(os.getcwd(), \"results\", \"answer_span_classifier\")\n",
    "save_path = os.path.join(model_path, \"fine_tuned\")\n",
    "patience = 2\n",
    "\n",
    "TRAIN = False # FLIP THIS TO TRAIN\n",
    "from transformers import EarlyStoppingCallback\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"No classifiers folder found, creating {model_path}...\")\n",
    "    os.makedirs(model_path)\n",
    "if not TRAIN:\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Model already exists at {save_path}, loading model...\")\n",
    "        model = BertForTokenClassification.from_pretrained(save_path, num_labels=3)\n",
    "        mbert_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "    else:\n",
    "        print(\"No model found.\")\n",
    "if TRAIN:\n",
    "    print(f\"No model found at {save_path}, training model...\")\n",
    "    model = BertForTokenClassification.from_pretrained(mbert_checkpoint, num_labels=3)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=mbert_tokenizer, # type: ignore\n",
    "        callbacks = [EarlyStoppingCallback(patience)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    print(\"Training completed.\")\n",
    "    model.save_pretrained(save_path) # type: ignore\n",
    "    mbert_tokenizer.save_pretrained(save_path) # type: ignore\n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bfe47",
   "metadata": {},
   "source": [
    "## First get a taste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd77ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_val_set = df_val.sample(15, shuffle=True) # get 50 random samples\n",
    "y_true, y_pred = get_results(model, smaller_val_set, mbert_tokenizer)\n",
    "display_results(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e249a",
   "metadata": {},
   "source": [
    "## Get Confusion Matrix for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_ar = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_val_ko = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_val_te = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "\n",
    "df_val_ar = df_val_ar.with_columns(\n",
    "    pl.struct([\"question\", \"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: bio_sequence_labeler(x[\"answer_start\"],x[\"answer\"],x[\"question\"],x[\"context\"],\n",
    "        mbert_tokenizer),\n",
    "    return_dtype=pl.List(pl.Int8)).alias(\"labels\"))\n",
    "df_val_ko = df_val_ko.with_columns(\n",
    "    pl.struct([\"question\", \"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: bio_sequence_labeler(x[\"answer_start\"],x[\"answer\"],x[\"question\"],x[\"context\"],\n",
    "        mbert_tokenizer),\n",
    "    return_dtype=pl.List(pl.Int8)).alias(\"labels\"))\n",
    "df_val_te = df_val_te.with_columns(\n",
    "    pl.struct([\"question\", \"context\", \"answer_start\", \"answer\"]).map_elements(\n",
    "        lambda x: bio_sequence_labeler(x[\"answer_start\"],x[\"answer\"],x[\"question\"],x[\"context\"],\n",
    "        mbert_tokenizer),\n",
    "    return_dtype=pl.List(pl.Int8)).alias(\"labels\"))\n",
    "\n",
    "y_true, y_pred = get_results(model, df_val_ar, mbert_tokenizer)\n",
    "display_results(y_true, y_pred, title=\"Arabic\")\n",
    "y_true, y_pred = get_results(model, df_val_ko, mbert_tokenizer)\n",
    "display_results(y_true, y_pred, title=\"Korean\")\n",
    "y_true, y_pred = get_results(model, df_val_te, mbert_tokenizer)\n",
    "display_results(y_true, y_pred, title=\"Telugu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe146b0",
   "metadata": {},
   "source": [
    "## Get full Confusion Matrix for fully trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0042e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true, y_pred = get_results(model, df_val, mbert_tokenizer)\n",
    "#display_results(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
