{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60819413",
   "metadata": {},
   "source": [
    "# mT5 fine-tuned for generative question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aee51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    MT5Tokenizer,\n",
    "    MT5ForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afbb87e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of MT5ForQuestionAnswering were not initialized from the model checkpoint at google/mt5-small and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "model = MT5ForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fee0f272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>question</th><th>context</th><th>lang</th><th>answerable</th><th>answer_start</th><th>answer</th><th>answer_inlang</th></tr><tr><td>str</td><td>str</td><td>str</td><td>bool</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;1990 నాటికి ఆఫ్రికాలో అతిపెద్ద…</td><td>&quot;various archipelagos. It conta…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;Nigeria&quot;</td><td>&quot;నైజీరియా&quot;</td></tr><tr><td>&quot;2010 నాటికీ వ్యవసాయ రంగంలో చైన…</td><td>&quot;A country with In [[2010]] Chi…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;the first&quot;</td><td>&quot;ప్రధమ&quot;</td></tr><tr><td>&quot;2011 నాటికి గొరిగపూడి గ్రామ జన…</td><td>&quot;Gorigapudi is a village belong…</td><td>&quot;te&quot;</td><td>true</td><td>306</td><td>&quot;2229&quot;</td><td>&quot;2229&quot;</td></tr><tr><td>&quot;2011 నాటికి పెద యాచవరం గ్రామ జ…</td><td>&quot;Peda Yachavaram is a village i…</td><td>&quot;te&quot;</td><td>true</td><td>247</td><td>&quot;4610&quot;</td><td>&quot;4610&quot;</td></tr><tr><td>&quot;ఆంధ్రప్రదేశ్ లో మొదటగా ఏ ఇంజనీ…</td><td>&quot;Andhra University College of E…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;Velagapudi Ramakrishna Siddhar…</td><td>&quot;వెలగపుడి రామకృష్ణ సిద్ధార్థ ఇం…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌───────────────┬───────────────┬──────┬────────────┬──────────────┬───────────────┬───────────────┐\n",
       "│ question      ┆ context       ┆ lang ┆ answerable ┆ answer_start ┆ answer        ┆ answer_inlang │\n",
       "│ ---           ┆ ---           ┆ ---  ┆ ---        ┆ ---          ┆ ---           ┆ ---           │\n",
       "│ str           ┆ str           ┆ str  ┆ bool       ┆ i64          ┆ str           ┆ str           │\n",
       "╞═══════════════╪═══════════════╪══════╪════════════╪══════════════╪═══════════════╪═══════════════╡\n",
       "│ 1990 నాటికి      ┆ various       ┆ te   ┆ false      ┆ -1           ┆ Nigeria       ┆ నైజీరియా          │\n",
       "│ ఆఫ్రికాలో అతిపెద్ద…  ┆ archipelagos. ┆      ┆            ┆              ┆               ┆               │\n",
       "│               ┆ It conta…     ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2010 నాటికీ      ┆ A country     ┆ te   ┆ false      ┆ -1           ┆ the first     ┆ ప్రధమ          │\n",
       "│ వ్యవసాయ రంగంలో   ┆ with In       ┆      ┆            ┆              ┆               ┆               │\n",
       "│ చైన…           ┆ [[2010]] Chi… ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2011 నాటికి      ┆ Gorigapudi is ┆ te   ┆ true       ┆ 306          ┆ 2229          ┆ 2229          │\n",
       "│ గొరిగపూడి గ్రామ    ┆ a village     ┆      ┆            ┆              ┆               ┆               │\n",
       "│ జన…           ┆ belong…       ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2011 నాటికి పెద   ┆ Peda          ┆ te   ┆ true       ┆ 247          ┆ 4610          ┆ 4610          │\n",
       "│ యాచవరం గ్రామ జ…  ┆ Yachavaram is ┆      ┆            ┆              ┆               ┆               │\n",
       "│               ┆ a village i…  ┆      ┆            ┆              ┆               ┆               │\n",
       "│ ఆంధ్రప్రదేశ్ లో    ┆ Andhra        ┆ te   ┆ false      ┆ -1           ┆ Velagapudi    ┆ వెలగపుడి రామకృష్ణ │\n",
       "│ మొదటగా ఏ ఇంజనీ…  ┆ University    ┆      ┆            ┆              ┆ Ramakrishna   ┆ సిద్ధార్థ ఇం…     │\n",
       "│               ┆ College of E… ┆      ┆            ┆              ┆ Siddhar…      ┆               │\n",
       "└───────────────┴───────────────┴──────┴────────────┴──────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5b3a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "def prepare_data(df: pl.DataFrame) -> Dataset:\n",
    "    # Convert Polars to dict format for HF datasets\n",
    "    data_dict = {\n",
    "        \"question\": df[\"question\"].to_list(),\n",
    "        \"context\": df[\"context\"].to_list(),\n",
    "        \"answers\": df[\"answer_inlang\"].to_list(),\n",
    "    }\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples: Dataset, tokenizer: AutoTokenizer):\n",
    "    # Tokenize with question and content separated by [SEP]\n",
    "    # [CLS] is added automatically\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18247e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qa_mt5(\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_val: Dataset,\n",
    "    model_checkpoint: str = \"google/mt5-small\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> tuple[MT5ForQuestionAnswering, MT5Tokenizer]:\n",
    "    # Load model\n",
    "    qa_generator = MT5ForQuestionAnswering.from_pretrained(model_checkpoint).to(device)\n",
    "    # Load tokenizer (mostly for saving complete model later)\n",
    "    tokenizer = MT5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        # Regularization\n",
    "        weight_decay=0.01,\n",
    "        # Memory settings\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=True,\n",
    "        # Evaluation\n",
    "        per_device_eval_batch_size=8,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=qa_generator,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "    )\n",
    "    # Clear torch cache before training\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Train and save the model\n",
    "    print(\"Training mBERT classifier...\")\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(f\"Environment variable set: {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "    trainer.train()\n",
    "\n",
    "    return qa_generator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91335b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e997f6bcba4481d8c7188068a72c431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2890c255b5514029b4df6e07c6cf56e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForQuestionAnswering were not initialized from the model checkpoint at google/mt5-small and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m tokenized_train = train_dataset.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize_function(x, tokenizer), batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m tokenized_val = val_dataset.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize_function(x, tokenizer), batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model, tokenizer = \u001b[43mtrain_qa_mt5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain_qa_mt5\u001b[39m\u001b[34m(tokenized_train, tokenized_val, model_checkpoint, device)\u001b[39m\n\u001b[32m     31\u001b[39m trainer = Trainer(\n\u001b[32m     32\u001b[39m     model=qa_generator,\n\u001b[32m     33\u001b[39m     args=training_args,\n\u001b[32m     34\u001b[39m     train_dataset=tokenized_train,\n\u001b[32m     35\u001b[39m     eval_dataset=tokenized_val,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Clear torch cache before training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mgc\u001b[49m.collect()\n\u001b[32m     39\u001b[39m torch.cuda.empty_cache()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare_data(df_te_train)\n",
    "val_dataset = prepare_data(df_te_val)\n",
    "tokenized_train = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "model, tokenizer = train_qa_mt5(tokenized_train, tokenized_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
