{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60819413",
   "metadata": {},
   "source": [
    "# mT5 fine-tuned for generative question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aee51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    MT5Tokenizer,\n",
    "    MT5ForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "model = MT5ForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af835487",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "# target is \"nice puppet\"\n",
    "target_start_index = torch.tensor([14])\n",
    "target_end_index = torch.tensor([15])\n",
    "\n",
    "outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "loss = outputs.loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "def prepare_data(df: pl.DataFrame) -> Dataset:\n",
    "    # Convert Polars to dict format for HF datasets\n",
    "    data_dict = {\n",
    "        \"question\": df[\"question\"].to_list(),\n",
    "        \"context\": df[\"context\"].to_list(),\n",
    "        \"answers\": df[\"answer_inlang\"].to_list(),\n",
    "    }\n",
    "    return Dataset.from_dict(data_dict)\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples: Dataset, tokenizer: AutoTokenizer):\n",
    "    # Tokenize with question and content separated by [SEP]\n",
    "    # [CLS] is added automatically\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18247e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qa_mt5(\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_val: Dataset,\n",
    "    model_checkpoint: str = \"google/mt5-small\",\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> tuple[MT5ForQuestionAnswering, MT5Tokenizer]:\n",
    "    # Load model\n",
    "    qa_generator = MT5ForQuestionAnswering.from_pretrained(model_checkpoint).to(device)\n",
    "    # Load tokenizer (mostly for saving complete model later)\n",
    "    tokenizer = MT5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        # Regularization\n",
    "        weight_decay=0.01,\n",
    "        # Memory settings\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=True,\n",
    "        # Evaluation\n",
    "        per_device_eval_batch_size=8,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=qa_generator,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "    )\n",
    "    # Clear torch cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "    # Train and save the model\n",
    "    print(\"Training mBERT classifier...\")\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(f\"Environment variable set: {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "    trainer.train()\n",
    "\n",
    "    return qa_generator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91335b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = prepare_data(df_te_train)\n",
    "val_dataset = prepare_data(df_te_val)\n",
    "tokenized_train = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "tokenized_val = val_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "model, tokenizer = train_qa_mt5(tokenized_train, tokenized_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
