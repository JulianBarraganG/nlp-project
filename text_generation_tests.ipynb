{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08c80ae",
   "metadata": {},
   "source": [
    "# MT5 Fine-tuning for Question Answering\n",
    "\n",
    "This notebook demonstrates fine-tuning the MT5 model on multilingual question answering data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d692cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "mt5_tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "mt5_model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "#mt5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ad4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "#t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "#t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca090855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import MBartForConditionalGeneration, AutoTokenizer\n",
    "#mbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "#mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#gpt2_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "#gpt2_model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(promt: str, model, tokenizer, **kwargs) -> str:\n",
    "    input_tokens = tokenizer(promt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    generated_tokens = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens=64,\n",
    "        **kwargs\n",
    "    )\n",
    "    answer = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best way to fall asleep?\"\n",
    "context = \"Do not drink a white monster right before going to bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer_question(f\"question: {question}  context: {context}\", t5_model, t5_tokenizer, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(f\"question: {question}  context: {context}\", mt5_model, mt5_tokenizer,  early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2326c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"What is the best way to fall asleep?\"\n",
    "#context = \"Do not drink a white monster right before going to bed\"\n",
    "#answer_question(f\"Answer the following question based on the given context. \\n Context: {context} \\n Question: {question}\", mbart_model, mbart_tokenizer, \n",
    "#    forced_bos_token_id=mbart_tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "#    early_stopping=True,\n",
    "#    num_beams=4,\n",
    "#    length_penalty=1.2,\n",
    "#    temperature=0.7,\n",
    "#    top_p=0.92,\n",
    "#    top_k=50,\n",
    "#    repetition_penalty=1.3,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer_question(f\"Answer the following question based on the given context. \\n Context: {context} \\n Question: {question}\", gpt2_model, gpt2_tokenizer, \n",
    "#    early_stopping=True,\n",
    "#    temperature=0.7,\n",
    "#    top_p=0.92,  \n",
    "#    top_k=50,\n",
    "#    repetition_penalty=1.2,\n",
    "#    no_repeat_ngram_size=2\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(df: pl.DataFrame):\n",
    "    df = df.with_columns([\n",
    "        (pl.lit(\"Question: \") + pl.col(\"question\") + pl.lit(\"\\n Context: \") + pl.col(\"context\")).alias(\"prompt\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def tokenize_to_dataset(df: pl.DataFrame, tokenizer, question_col: str = \"prompt\", answer_col: str = \"answer_inlang\"):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        df[question_col].to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        df[answer_col].to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict({k: v.numpy() for k, v in dataset_dict.items()})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te_train_prompt = generate_prompts(df_te_train)\n",
    "df_te_val_prompt = generate_prompts(df_te_val)\n",
    "\n",
    "train_dataset = tokenize_to_dataset(df_te_train_prompt, mt5_tokenizer)\n",
    "val_dataset = tokenize_to_dataset(df_te_val_prompt, mt5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cebb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/llm-course/chapter7/4?fw=pt\n",
    "\n",
    "model_save_dir = os.path.join(\"results\", \"mt5-telugu-qa\")\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_save_dir,\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=6,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    log_level=\"info\",\n",
    "    report_to=[],\n",
    "    logging_dir=None\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(mt5_tokenizer, model=mt5_model)\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer=mt5_tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=mt5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=mt5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"results/mt5-telugu-qa/fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af534c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"results/mt5-telugu-qa/fine_tuned\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"results/mt5-telugu-qa/fine_tuned\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df_te_val_prompt[\"prompt\"][0]\n",
    "answer = df_te_val_prompt[\"answer_inlang\"][0]\n",
    "\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, early_stopping=True)\n",
    "gen_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Generated Answer: {gen_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
