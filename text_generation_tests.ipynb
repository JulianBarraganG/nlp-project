{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08c80ae",
   "metadata": {},
   "source": [
    "# MT5 Fine-tuning for Question Answering\n",
    "\n",
    "This notebook demonstrates fine-tuning the MT5 model on multilingual question answering data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e152c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bb3846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>question</th><th>context</th><th>lang</th><th>answerable</th><th>answer_start</th><th>answer</th><th>answer_inlang</th></tr><tr><td>str</td><td>str</td><td>str</td><td>bool</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;1990 నాటికి ఆఫ్రికాలో అతిపెద్ద…</td><td>&quot;various archipelagos. It conta…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;Nigeria&quot;</td><td>&quot;నైజీరియా&quot;</td></tr><tr><td>&quot;2010 నాటికీ వ్యవసాయ రంగంలో చైన…</td><td>&quot;A country with In [[2010]] Chi…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;the first&quot;</td><td>&quot;ప్రధమ&quot;</td></tr><tr><td>&quot;2011 నాటికి గొరిగపూడి గ్రామ జన…</td><td>&quot;Gorigapudi is a village belong…</td><td>&quot;te&quot;</td><td>true</td><td>306</td><td>&quot;2229&quot;</td><td>&quot;2229&quot;</td></tr><tr><td>&quot;2011 నాటికి పెద యాచవరం గ్రామ జ…</td><td>&quot;Peda Yachavaram is a village i…</td><td>&quot;te&quot;</td><td>true</td><td>247</td><td>&quot;4610&quot;</td><td>&quot;4610&quot;</td></tr><tr><td>&quot;ఆంధ్రప్రదేశ్ లో మొదటగా ఏ ఇంజనీ…</td><td>&quot;Andhra University College of E…</td><td>&quot;te&quot;</td><td>false</td><td>-1</td><td>&quot;Velagapudi Ramakrishna Siddhar…</td><td>&quot;వెలగపుడి రామకృష్ణ సిద్ధార్థ ఇం…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "┌───────────────┬───────────────┬──────┬────────────┬──────────────┬───────────────┬───────────────┐\n",
       "│ question      ┆ context       ┆ lang ┆ answerable ┆ answer_start ┆ answer        ┆ answer_inlang │\n",
       "│ ---           ┆ ---           ┆ ---  ┆ ---        ┆ ---          ┆ ---           ┆ ---           │\n",
       "│ str           ┆ str           ┆ str  ┆ bool       ┆ i64          ┆ str           ┆ str           │\n",
       "╞═══════════════╪═══════════════╪══════╪════════════╪══════════════╪═══════════════╪═══════════════╡\n",
       "│ 1990 నాటికి      ┆ various       ┆ te   ┆ false      ┆ -1           ┆ Nigeria       ┆ నైజీరియా          │\n",
       "│ ఆఫ్రికాలో అతిపెద్ద…  ┆ archipelagos. ┆      ┆            ┆              ┆               ┆               │\n",
       "│               ┆ It conta…     ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2010 నాటికీ      ┆ A country     ┆ te   ┆ false      ┆ -1           ┆ the first     ┆ ప్రధమ          │\n",
       "│ వ్యవసాయ రంగంలో   ┆ with In       ┆      ┆            ┆              ┆               ┆               │\n",
       "│ చైన…           ┆ [[2010]] Chi… ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2011 నాటికి      ┆ Gorigapudi is ┆ te   ┆ true       ┆ 306          ┆ 2229          ┆ 2229          │\n",
       "│ గొరిగపూడి గ్రామ    ┆ a village     ┆      ┆            ┆              ┆               ┆               │\n",
       "│ జన…           ┆ belong…       ┆      ┆            ┆              ┆               ┆               │\n",
       "│ 2011 నాటికి పెద   ┆ Peda          ┆ te   ┆ true       ┆ 247          ┆ 4610          ┆ 4610          │\n",
       "│ యాచవరం గ్రామ జ…  ┆ Yachavaram is ┆      ┆            ┆              ┆               ┆               │\n",
       "│               ┆ a village i…  ┆      ┆            ┆              ┆               ┆               │\n",
       "│ ఆంధ్రప్రదేశ్ లో    ┆ Andhra        ┆ te   ┆ false      ┆ -1           ┆ Velagapudi    ┆ వెలగపుడి రామకృష్ణ │\n",
       "│ మొదటగా ఏ ఇంజనీ…  ┆ University    ┆      ┆            ┆              ┆ Ramakrishna   ┆ సిద్ధార్థ ఇం…     │\n",
       "│               ┆ College of E… ┆      ┆            ┆              ┆ Siddhar…      ┆               │\n",
       "└───────────────┴───────────────┴──────┴────────────┴──────────────┴───────────────┴───────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\", pl.col(\"answer_inlang\").is_not_null())\n",
    "df_te_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d1a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e66903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from disk\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "mt5_telugu_path = os.path.join(os.path.join(\"results\", \"mt5-telugu-qa\"))\n",
    "\n",
    "if os.path.exists(os.path.join(mt5_telugu_path, \"fine_tuned\")):\n",
    "    print(\"Loading model from disk\")\n",
    "    mt5_tokenizer = T5Tokenizer.from_pretrained(os.path.join(mt5_telugu_path, \"fine_tuned\"))\n",
    "    mt5_model = MT5ForConditionalGeneration.from_pretrained(os.path.join(mt5_telugu_path, \"fine_tuned\"))\n",
    "    mt5_model.to(device)\n",
    "else:\n",
    "    print(\"Loading model from Huggingface\")\n",
    "    mt5_tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "    mt5_model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "    mt5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d692cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "#mt5_tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "#mt5_model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "#mt5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131ad4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "#t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "#t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca090855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import MBartForConditionalGeneration, AutoTokenizer\n",
    "#mbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "#mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd8d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#gpt2_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "#gpt2_model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83d38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(promt: str, model, tokenizer, **kwargs) -> str:\n",
    "    input_tokens = tokenizer(promt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    generated_tokens = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens=64,\n",
    "        **kwargs\n",
    "    )\n",
    "    answer = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da23119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the best way to fall asleep?\"\n",
    "context = \"Do not drink a white monster right before going to bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "522f4f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0>?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(f\"question: {question}  context: {context}\", mt5_model, mt5_tokenizer,  early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef0eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer_question(f\"question: {question}  context: {context}\", t5_model, t5_tokenizer, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2326c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"What is the best way to fall asleep?\"\n",
    "#context = \"Do not drink a white monster right before going to bed\"\n",
    "#answer_question(f\"Answer the following question based on the given context. \\n Context: {context} \\n Question: {question}\", mbart_model, mbart_tokenizer, \n",
    "#    forced_bos_token_id=mbart_tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "#    early_stopping=True,\n",
    "#    num_beams=4,\n",
    "#    length_penalty=1.2,\n",
    "#    temperature=0.7,\n",
    "#    top_p=0.92,\n",
    "#    top_k=50,\n",
    "#    repetition_penalty=1.3,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ec3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer_question(f\"Answer the following question based on the given context. \\n Context: {context} \\n Question: {question}\", gpt2_model, gpt2_tokenizer, \n",
    "#    early_stopping=True,\n",
    "#    temperature=0.7,\n",
    "#    top_p=0.92,  \n",
    "#    top_k=50,\n",
    "#    repetition_penalty=1.2,\n",
    "#    no_repeat_ngram_size=2\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5bbe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(df: pl.DataFrame):\n",
    "    #df = df.with_columns([\n",
    "    #    (pl.lit(\"Question: \") + pl.col(\"question\") + pl.lit(\"\\n Context: \") + pl.col(\"context\")).alias(\"prompt\")\n",
    "    #])\n",
    "    df = df.with_columns([pl.col(\"question\").alias(\"prompt\")])\n",
    "    return df\n",
    "\n",
    "def tokenize_to_dataset(df: pl.DataFrame, tokenizer, question_col: str = \"prompt\", answer_col: str = \"answer_inlang\"):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        df[question_col].to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        df[answer_col].to_list(),\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict({k: v.numpy() for k, v in dataset_dict.items()})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b38c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te_train_prompt = generate_prompts(df_te_train)\n",
    "df_te_val_prompt = generate_prompts(df_te_val)\n",
    "\n",
    "train_dataset = tokenize_to_dataset(df_te_train_prompt, mt5_tokenizer)\n",
    "val_dataset = tokenize_to_dataset(df_te_val_prompt, mt5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cebb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asge1\\AppData\\Local\\Temp\\ipykernel_17624\\3798707823.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/learn/llm-course/chapter7/4?fw=pt\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "epochs = 16\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=mt5_telugu_path,\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=6,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=epochs,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "\n",
    "    save_total_limit=5,\n",
    "    save_strategy = \"best\",\n",
    "    load_best_model_at_end = True,\n",
    "\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = epochs * 5,\n",
    "    log_level=\"info\",\n",
    "    report_to=[],\n",
    "    logging_dir=None\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(mt5_tokenizer, model=mt5_model)\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer=mt5_tokenizer):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=mt5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=mt5_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97cb50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 50\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 208\n",
      "  Number of trainable parameters = 300,176,768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 02:38, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>17.457400</td>\n",
       "      <td>14.886859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>17.345900</td>\n",
       "      <td>14.460106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-13\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-13\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-13\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-13\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-13\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-13\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-195] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-26\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-26\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-26\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-26\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-26\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-26\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-208] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-39\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-39\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-39\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-39\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-39\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-39\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-13] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-52\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-52\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-52\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-52\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-52\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-52\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-26] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-65\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-65\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-65\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-65\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-65\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-65\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-39] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-78\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-78\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-78\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-78\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-78\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-78\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-52] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-91\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-91\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-91\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-91\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-91\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-91\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-65] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-104\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-104\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-104\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-104\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-104\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-104\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-78] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-117\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-117\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-117\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-117\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-117\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-117\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-91] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-130\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-130\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-130\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-130\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-130\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-130\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-104] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-143\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-143\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-143\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-143\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-143\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-143\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-117] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-156\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-156\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-156\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-156\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-156\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-156\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-130] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 6\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-169\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-169\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-169\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-169\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-169\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-169\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-143] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-182\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-182\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-182\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-182\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-182\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-182\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-156] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-195\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-195\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-195\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-195\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-195\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-195\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-169] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\checkpoint-208\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-208\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\checkpoint-208\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\checkpoint-208\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\checkpoint-208\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\checkpoint-208\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\mt5-telugu-qa\\checkpoint-182] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to results\\mt5-telugu-qa\\fine_tuned\n",
      "Configuration saved in results\\mt5-telugu-qa\\fine_tuned\\config.json\n",
      "Configuration saved in results\\mt5-telugu-qa\\fine_tuned\\generation_config.json\n",
      "Model weights saved in results\\mt5-telugu-qa\\fine_tuned\\model.safetensors\n",
      "tokenizer config file saved in results\\mt5-telugu-qa\\fine_tuned\\tokenizer_config.json\n",
      "Special tokens file saved in results\\mt5-telugu-qa\\fine_tuned\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(os.path.join(mt5_telugu_path, \"fine_tuned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe67ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: మలేరియా వ్యాధి కి మందు కనిపెట్టిన శాస్త్రవేత్త ఎవరు?\n",
      "Answer: హన్స్ ఆండర్సాగ్\n",
      "Generated Answer: <extra_id_0>.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(os.path.join(mt5_telugu_path, \"fine_tuned\"))\n",
    "model = MT5ForConditionalGeneration.from_pretrained(os.path.join(mt5_telugu_path, \"fine_tuned\"))\n",
    "model.to(device)\n",
    "\n",
    "question = df_te_val_prompt[\"prompt\"][0]\n",
    "answer = df_te_val_prompt[\"answer_inlang\"][0]\n",
    "\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, early_stopping=True)\n",
    "gen_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Generated Answer: {gen_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
