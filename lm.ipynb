{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d681fbb",
   "metadata": {},
   "source": [
    "## Weak 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get huggingface dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = pl.from_pandas(dataset[\"train\"].to_pandas())\n",
    "df_val = pl.from_pandas(dataset[\"validation\"].to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_train.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ko = df_train.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_te = df_train.filter(pl.col(\"lang\") == \"te\")\n",
    "df_arkote = df_train.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))\n",
    "assert df_ar.height + df_ko.height + df_te.height == df_arkote.height; # sanity check\n",
    "\n",
    "df_ar_val = df_val.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ko_val = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_arkote_val = df_val.filter(pl.col(\"lang\").is_in([\"ar\", \"ko\", \"te\"]))\n",
    "assert df_ar_val.height + df_ko_val.height + df_te_val.height == df_arkote_val.height; # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arkote.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ed480",
   "metadata": {},
   "source": [
    "### Get tokenizers and look at sample (Arabic) sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual BERT tokenizer\n",
    "mbert_tokenizer = Tokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "# Load GPT-4 tokenizer\n",
    "gpt4_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Load NLLB-200 tokenizer\n",
    "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization example\n",
    "\" | \".join(mbert_tokenizer.encode(df_te[\"question\"][0]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a80d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode each token from GPT-4 tokenizer\n",
    "\" | \".join([gpt4_tokenizer.decode([token]) for token in gpt4_tokenizer.encode(df_te[\"question\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e331fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB-200 tokenization example\n",
    "\" | \".join(nllb_tokenizer.tokenize(df_te[\"question\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955647f",
   "metadata": {},
   "source": [
    "## Get the top 5 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language dict\n",
    "lang_dict = {\n",
    "    \"ar\": \"arb_Arab\",\n",
    "    \"ko\": \"kor_Hang\",\n",
    "    \"te\": \"tel_Telu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up translation pipeline for NLLB-200\n",
    "translator_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a606491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example translation\n",
    "random_txt = df_ko[\"question\"][0]\n",
    "translator = translator = pipeline(\n",
    "        \"translation\",\n",
    "        model=translator_model,\n",
    "        tokenizer=nllb_tokenizer,\n",
    "        src_lang=\"kor_Hang\",\n",
    "        tgt_lang=\"eng_Latn\",\n",
    "    )\n",
    "translator(random_txt)[0][\"translation_text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_translator(src_lang:str):\n",
    "    translator = pipeline(\n",
    "        \"translation\",\n",
    "        model=translator_model,\n",
    "        tokenizer=nllb_tokenizer,\n",
    "        src_lang=src_lang,\n",
    "        tgt_lang=\"eng_Latn\",\n",
    "    )\n",
    "    return translator\n",
    "\n",
    "def tokenize_question(df: pl.DataFrame, use_cache: bool = True, translation: bool = True, tokenization: bool = True) -> pl.DataFrame:\n",
    "    \n",
    "    # Check if already tokenized and translated\n",
    "    src_lang = lang_dict[df[\"lang\"][0]]\n",
    "    cache_path = os.path.join(\"data\", f\"tydi_xor_rc_{src_lang}\")\n",
    "    print(f\"Tokenizing and translating {src_lang}...\")\n",
    "    if use_cache and f\"tydi_xor_rc_{src_lang}.parquet\" in os.listdir(\"data\"):\n",
    "        # Load from parquet\n",
    "        print(f\"Loading from cached file tydi_xor_rc_{src_lang}.parquet\")\n",
    "\n",
    "        return pl.read_parquet(os.path.join(\"data\", f\"tydi_xor_rc_{src_lang}.parquet\"))\n",
    "\n",
    "    if tokenization:\n",
    "        # Tokenize questions using multilingual BERT tokenizer\n",
    "        print(\"Tokenizing\")\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"question\")\n",
    "            .map_elements(lambda x: nllb_tokenizer.tokenize(x), return_dtype=pl.List(pl.Utf8))\n",
    "            .alias(\"tokens\")\n",
    "        )\n",
    "\n",
    "    if translation:\n",
    "        print(\"Translating\")\n",
    "        # Translate questions using NLLB-200\n",
    "        translator = _make_translator(src_lang)\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"question\")\n",
    "            .map_elements(lambda x: translator(x)[0][\"translation_text\"])\n",
    "            .alias(\"translation\")\n",
    "        )\n",
    "\n",
    "    if use_cache:\n",
    "        print(f\"Caching to {cache_path}.parquet and {cache_path}.xlsx\")\n",
    "        df.write_parquet(cache_path + \".parquet\")\n",
    "        df.write_excel(cache_path + \".xlsx\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e811d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_question(df_ar)\n",
    "tokenize_question(df_ko)\n",
    "tokenize_question(df_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Get the number of questions and the number of total tokens in each language\n",
    "def df_metadata(df: pl.DataFrame) -> None:\n",
    "    n_questions = df.height\n",
    "    tokens_list = tokenize_question(df, use_cache = False, translation=False)[\"tokens\"].to_list()\n",
    "    n_tokens = sum([len(tokens) for tokens in tokens_list])\n",
    "    print(f\"Number of questions: {n_questions} Number of tokens: {n_tokens}\")\n",
    "\n",
    "print(\"Arabic dataset\")\n",
    "df_metadata(df_ar)\n",
    "print(\"Korean dataset\")\n",
    "df_metadata(df_ko)\n",
    "print(\"Telugu dataset\")\n",
    "df_metadata(df_te)\n",
    "print(\"Arabic validation dataset\")\n",
    "df_metadata(df_ar_val)\n",
    "print(\"Korean validation dataset\")\n",
    "df_metadata(df_ko_val)\n",
    "print(\"Telugu validation dataset\")\n",
    "df_metadata(df_te_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and translate each language dataframe, then compute token frequencies\n",
    "for df in [df_ar, df_ko, df_te]:\n",
    "    df = tokenize_question(df)\n",
    "    count_dict = {}\n",
    "    for tokens in df[\"tokens\"]:\n",
    "        for token in tokens:\n",
    "            if token in count_dict:\n",
    "                count_dict[token] += 1\n",
    "            else:\n",
    "                count_dict[token] = 1\n",
    "    sorted_frequency_list = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_frequency_list[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and translate each language dataframe, then compute token frequencies of the translations\n",
    "for df in [df_ar, df_ko, df_te]:\n",
    "    df = tokenize_question(df)\n",
    "    count_dict = {}\n",
    "    for translation in df[\"translation\"]:\n",
    "        for token in nllb_tokenizer.tokenize(translation):\n",
    "            if token in count_dict:\n",
    "                count_dict[token] += 1\n",
    "            else:\n",
    "                count_dict[token] = 1\n",
    "    sorted_frequency_list = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_frequency_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9743ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top tokens in answerable true and false\n",
    "for df in [df_ar, df_ko, df_te]:\n",
    "    df = tokenize_question(df)\n",
    "    for answerable in [True, False]:\n",
    "        count_dict = {}\n",
    "        for translation in df.filter(pl.col(\"answerable\") == answerable)[\"translation\"]:\n",
    "            for token in nllb_tokenizer.tokenize(translation):\n",
    "                if token in count_dict:\n",
    "                    count_dict[token] += 1\n",
    "                else:\n",
    "                    count_dict[token] = 1\n",
    "        sorted_frequency_list = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Top tokens in {lang_dict[df['lang'][0]]} answerable={answerable}: {sorted_frequency_list[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
