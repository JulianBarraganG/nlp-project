{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bf2d7d",
   "metadata": {},
   "source": [
    "# LM for QA Tidy_XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5847f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bert_utils import (\n",
    "    predict_binary,\n",
    "    prepare_data,\n",
    "    tokenize_function,\n",
    "    train_mbert,\n",
    ")\n",
    "\n",
    "# Huggingface imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c187f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for training\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "df_train = dataset[\"train\"].to_polars()\n",
    "df_val = dataset[\"validation\"].to_polars()\n",
    "\n",
    "# Arabic, Telegu and Korean\n",
    "df_ar_train = df_train.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_ar_val = df_val.filter(pl.col(\"lang\") == \"ar\")\n",
    "df_te_train = df_train.filter(pl.col(\"lang\") == \"te\")\n",
    "df_te_val = df_val.filter(pl.col(\"lang\") == \"te\")\n",
    "df_ko_train = df_train.filter(pl.col(\"lang\") == \"ko\")\n",
    "df_ko_val = df_val.filter(pl.col(\"lang\") == \"ko\")\n",
    "\n",
    "# Make a dict\n",
    "data = {\n",
    "    \"arabic\": {\"train\": df_ar_train, \"val\": df_ar_val},\n",
    "    \"telugu\": {\"train\": df_te_train, \"val\": df_te_val},\n",
    "    \"korean\": {\"train\": df_ko_train, \"val\": df_ko_val},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50abbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Arabic distribution train\n",
    "print(f\"Arabic TRAINING set size: {len(df_ar_train)} with a total of {df_ar_train['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_ar_train['answerable'].sum() / len(df_ar_train) * 100:.2f}% answerable questions.\")    \n",
    "# Check Arabic distribution val\n",
    "print(f\"Arabic VALIDATION set size: {len(df_ar_val)} with a total of {df_ar_val['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_ar_val['answerable'].sum() / len(df_ar_val) * 100:.2f}% answerable questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Telegu distribution train\n",
    "print(f\"Telegu TRAINING set size: {len(df_te_train)} with a total of {df_te_train['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_te_train['answerable'].sum() / len(df_te_train) * 100:.2f}% answerable questions.\")    \n",
    "# Check Telegu distribution val\n",
    "print(f\"Telegu VALIDATION set size: {len(df_te_val)} with a total of {df_te_val['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_te_val['answerable'].sum() / len(df_te_val) * 100:.2f}% answerable questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Korean distribution train\n",
    "print(f\"Korean TRAINING set size: {len(df_ko_train)} with a total of {df_ko_train['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_ko_train['answerable'].sum() / len(df_ko_train) * 100:.2f}% answerable questions.\")    \n",
    "# Check Korean distribution val\n",
    "print(f\"Korean VALIDATION set size: {len(df_ko_val)} with a total of {df_ko_val['answerable'].sum()} answerable questions.\")\n",
    "print(f\"This gives a distribution of {df_ko_val['answerable'].sum() / len(df_ko_val) * 100:.2f}% answerable questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5c62f",
   "metadata": {},
   "source": [
    "## Finetune the multilingual BERT for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE OF DATA PROCESS PIPELINE\n",
    "# Prepare datasets\n",
    "# train_dataset = prepare_data(data[\"telugu\"][\"train\"])\n",
    "# # Tokenize datasets - fix the function call\n",
    "# tokenized_train = train_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n",
    "# tokenized_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "all_tokenizers = {} # they're all the same\n",
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "for lang in [\"arabic\", \"telugu\", \"korean\"]:\n",
    "    cap_lang = lang.capitalize()\n",
    "    print(f\"\\n--- Processing language: {cap_lang} ---\")\n",
    "    trained = False\n",
    "    classifiers_dir = \"./mbert_classifiers\"\n",
    "    save_path = f\"{lang}_mbert_answerable_classifier\"\n",
    "    full_save_path = os.path.join(classifiers_dir, save_path)\n",
    "    # Check if model exists\n",
    "    if not os.path.exists(classifiers_dir):\n",
    "        print(f\"No classifiers folder found, creating {classifiers_dir}...\")\n",
    "        os.makedirs(classifiers_dir)\n",
    "    if os.path.exists(full_save_path):\n",
    "        print(f\"Found existing model for {cap_lang}, loading...\")\n",
    "        all_classifiers[lang] = AutoModelForSequenceClassification.from_pretrained(full_save_path)\n",
    "        all_tokenizers[lang] = AutoTokenizer.from_pretrained(full_save_path) # all the same, we don't train tokenizer\n",
    "        trained = True\n",
    "        print(f\"Model for {cap_lang} loaded.\")\n",
    "\n",
    "    # If model doesn't exist, train it\n",
    "    if not trained:\n",
    "        print(\"Model not found, training new mBERT model...\")\n",
    "        mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)\n",
    "        all_tokenizers[lang] = mbert_tokenizer # all the same, we don't train tokenizer\n",
    "        mbert_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "            mbert_checkpoint,\n",
    "            num_labels=2,\n",
    "        )\n",
    "        # Prepare datasets\n",
    "        train_dataset = prepare_data(data[lang][\"train\"])\n",
    "        val_dataset = prepare_data(data[lang][\"val\"])\n",
    "\n",
    "        # Tokenize datasets - fix the function call\n",
    "        tokenized_train = train_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n",
    "        tokenized_val = val_dataset.map(lambda examples: tokenize_function(examples, mbert_tokenizer), batched=True)\n",
    "        # Train\n",
    "        classifier, tokenizer = train_mbert(\n",
    "            tokenized_train,\n",
    "            tokenized_val,\n",
    "            model_checkpoint = mbert_checkpoint,\n",
    "            device=device,\n",
    "        ) # type: ignore\n",
    "        print(\"Saving model...\")\n",
    "        classifier.save_pretrained(full_save_path) # type: ignore\n",
    "        tokenizer.save_pretrained(full_save_path) # type: ignore\n",
    "        print(f\"Model trained and saved to {full_save_path}.\")\n",
    "        # Store the trained model in notebook variable\n",
    "        all_classifiers[lang] = AutoModelForSequenceClassification.from_pretrained(full_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ea2da",
   "metadata": {},
   "source": [
    "## Compare pre-trained vs fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model and tokenizer\n",
    "pt_mbert = AutoModelForSequenceClassification.from_pretrained(\n",
    "    mbert_checkpoint,\n",
    "    num_labels=2,\n",
    ")\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)\n",
    "\n",
    "\n",
    "for lang in [\"arabic\", \"telugu\", \"korean\"]:\n",
    "    # Test on a few examples BEFORE training\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{lang.upper()} BEFORE FINE-TUNING (Random Classification Head)\")\n",
    "    print(\"=\" * 50)\n",
    "    # Get a few examples from your validation set\n",
    "    for i in range(3):\n",
    "        example = data[lang][\"val\"].row(i, named=True)\n",
    "        \n",
    "        result = predict_binary(example[\"question\"], example[\"context\"], pt_mbert, mbert_tokenizer)\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question: {example['question'][:100]}...\")\n",
    "        print(f\"Ground Truth: {'Answerable' if example['answerable'] else 'Not Answerable'}\")\n",
    "        print(f\"Prediction: {'Answerable' if result['prediction'] == 1 else 'Not Answerable'}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"Probs: [Not Answerable: {result['prob_class_0']:.3f}, Answerable: {result['prob_class_1']:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac65a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [\"arabic\", \"telugu\", \"korean\"]:\n",
    "    # Test AFTER training on the same examples\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"{lang.upper()} AFTER FINE-TUNING\")\n",
    "    print(\"=\" * 50)\n",
    "    for i in range(3):\n",
    "        example = data[lang][\"val\"].row(i, named=True)\n",
    "\n",
    "        result = predict_binary(example['question'], example['context'], all_classifiers[lang], mbert_tokenizer)\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question: {example['question'][:100]}...\")\n",
    "        print(f\"Ground Truth: {'Answerable' if example['answerable'] else 'Not Answerable'}\")\n",
    "        print(f\"Prediction: {'Answerable' if result['prediction'] == 1 else 'Not Answerable'}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc335623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the global accuracy for each language on validation set  \n",
    "for lang in [\"arabic\", \"telugu\", \"korean\"]:\n",
    "    correct = 0\n",
    "    total = len(data[lang][\"val\"])\n",
    "    print(f\"\\nCalculating accuracy for {lang} on validation set of size {total}...\")\n",
    "    for i in range(total):\n",
    "        example = data[lang][\"val\"].row(i, named=True)\n",
    "        result = predict_binary(example['question'], example['context'], all_classifiers[lang], mbert_tokenizer)\n",
    "        if result['prediction'] == example['answerable']:\n",
    "            correct += 1\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Accuracy for {lang} on validation set: {accuracy:.2f}% ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03352d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrices for each language\n",
    "for lang in [\"arabic\", \"telugu\", \"korean\"]:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total = len(data[lang][\"val\"])\n",
    "    print(f\"\\nCalculating confusion matrix for {lang} on validation set of size {total}...\")\n",
    "    for i in range(total):\n",
    "        example = data[lang][\"val\"].row(i, named=True)\n",
    "        result = predict_binary(example['question'], example['context'], all_classifiers[lang], mbert_tokenizer)\n",
    "        y_true.append(example['answerable'])\n",
    "        y_pred.append(result['prediction'])\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix for {lang.capitalize()}\")\n",
    "\n",
    "    plt.colorbar()\n",
    "    tick_marks = range(len(['Not Answerable', 'Answerable']))\n",
    "    plt.xticks(tick_marks, ['Not Answerable', 'Answerable'])\n",
    "    plt.yticks(tick_marks, ['Not Answerable', 'Answerable'])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    # Include numbers as text in the plot\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]:.2f}\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()\n",
    "    print(f\"Classification Report for {lang.capitalize()}:\\n{classification_report(y_true, y_pred, target_names=['Not Answerable', 'Answerable'])}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
